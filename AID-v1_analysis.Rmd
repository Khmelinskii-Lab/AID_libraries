---
title: "mNG-AID*-3Myc library"
author: "EG"
date: "February 22, 2024"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
editor_options: 
  chunk_output_type: console
---

# Introduction

This document contains the workflow for analysis of AID library v1 experiment in terms of fluorescence and size measurements.


# Setup - Fluorescence

```{r Knitr Setup}
#The below command can be added to the section above for a different output.
#runtime: shiny

#Setting the default knitr options for each chunk here.
#"warning" and "message" settings are set to F as the codes are pre-reviewed
#If running a new dataset, this can be changed to true to see all/any problems
#"error" is set to T so that the output is made and error checking can be done
knitr::opts_chunk$set(echo = TRUE, fig.width = 18, fig.height = 16,
                      warning = FALSE, message = FALSE, error = TRUE) 

```


```{r Setup}
#Setting working directory for working in R Studio
setwd("~/Downloads/AID_libraries_2_AIDv1")
PrimaryDirectory = getwd()

#Loading necessary packages for the analysis. Packages that are not obvious for its
#function have extra comments beside it.
library(tidyverse)
library(ggExtra) #Add-on to ggplot2; For ggMarginal function
library(ggrepel) #Add-on to ggplot2; Repel overlapping text labels
library(gghighlight) #Add-on to ggplot2; Data highlighting
library(grDevices) #Support package for base and grid graphics
library(gridExtra) #For creating grid plots
library(openxlsx)
library(kableExtra) #For knitr tables rendering
library(bootstrap) #For jackknife function
library(VennDiagram)
library(gplots) #for balloon plot
#library(ragp) #for phobius analysis
library(janitor)
library(patchwork)
library(gdata)

#Loading the environment data.


#For reproducibility, so that although the next process is random,
#it is randomised in the same way everytime.
set.seed(112358)

#Setting ggplot's default theme for all plots
theme_set(theme_light(base_size = 14))
theme_update(strip.text.x = element_text(size = rel(0.8), lineheight = 0.05,
                                         margin = margin(2,2,2,2)))
doc_theme <- theme_get()

```

```{r Setup 2}
#Setting destination for certain output folders
plots_folder <- '05_plots'
dataset_folder <- '06_datasets'

#This alphabet data is obtained from colonyHTS package. Needed for analysis.
alphabet1536 <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L",
                  "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X",
                  "Y", "Z", "AA", "AB", "AC", "AD", "AE", "AF")

alphabet1536_rev <- 1:length(alphabet1536)
names(alphabet1536_rev) <- alphabet1536

#Loading border colonies list
BorderColonies <- read.xlsx("06_datasets/BorderColoniesList.xlsx",
                            startRow = 1, colNames = TRUE, sheet = "Sheet1")

```

```{r Functions}
# Function needed for jackknife function (Taken from SGA Tools)
# @params x: a numeric vector
theta <- function(x){ sd(x, na.rm = T) }

#JKcalc <- function(x){
#  ((var(x, na.rm=T) * (length(x)-1)) - ((jackknife(x, theta)$jack.values)^2) * (length((jackknife(x, theta)$jack.values))-2)) > (0.995*(var(x, na.rm=T) * (length(x)-1))) }

JKcalc <- function(x){
  JK = jackknife(x, theta)$jack.values
  total.var = var(x, na.rm=T) * (length(x)-1)
  jk.var =  (JK^2) * (length(JK)-2)
  JK.TRUE = (total.var - jk.var) > (0.995*total.var)
  JK.TRUE
}

```


This document contains the workflow for analysis of the mNG-AID*-3Myc experiment

The screen consists of two libraries. There is protein degradation in one of the libraries but not in the other and it was performed only in one condition.

The screen is laid out in a format where each ORF has 16 position divided in 4 quadrants:
- The first 3 positions of the quadrant 1 and 2 have colonies where we expect to see degradation.
- The first 3 positions of the quadrant 3 have colonies where we expect no degradation, since there is no F-Box.
- The first 3 positions of the quadrant 4 have colonies without any fluorescence protein (His4-AID*-3Myc) as background reference
- The fourth position of all quadrants had a reference fluorescence colony, the PDC1-mNG-AID*-3Myc without F-Box.

```{r Experimental conditions 2}
#Loading the Experiment's annotation file
load("20230904_mNG_AID_anno.rda")

```

# Raw Data Processing

The following analysis uses the gain adjusted data.

## Removal of border colonies

The border colonies are not taken into account for analysis, and so, rows A-D & AC-AF and columns 1-4 & 45-48 are removed.

```{r Removing border colonies}
load("dataTableRaw_GainAdj.rda")

#Removing border colonies
df1 <- anti_join(df, BorderColonies) #%>%

#Note: Other than the border (4 rows and columns on either side of the 1536 array),
# columns 5-8 and 41-44 (1536 format) are also filled with the same border strains.
# The strains in these columns are labelled as "BORDER" in the ORF column.
# Exception: the REF strains are still labelled as TPI1 in the ORF column even if it is from these columns.
# These are not filtered off here yet because they contribute to the colony size
# normalizing for the colony size filter step.
#Note 2: There was a measurement done for just an empty plate with no colony.
# I initially did this to see if there was an issue with the TECAN for the odd
# measurement outcome, but I don't see anything so I remove it now.

str(df1)

```


## Intial filtering by Colony size

The data contains information on colony size on the measurement plate step.

The data is first grouped by plate, then colony sizes are normalised to the median of each plate.

```{r Colony size normalisation}
#Calculate the median colony size, and divide each value by the median to normalise
df_colony_filtered <- df1 %>%
  group_by(plate) %>%
  mutate(areaImgcentered = (sizeImg / median(sizeImg))) %>%
  ungroup() %>%
  filter(between(col_1536, 5, 44))
#Note: After normalising the colony size, then I remove the non-sample strains

```


```{r}

#Heatmap
HM <- df_colony_filtered %>%
  filter(plate == "p01") %>%
  select(row_1536, col_1536, mNG) %>%
  filter(row_1536 == "U" | row_1536 == "V" | row_1536 == "W" | row_1536 == "X") %>%
  filter(col_1536 %in% c(5:16) )

HM <- HM[order(HM$row_1536),]

#Figure 2c
Im2c <- ggplot(HM, aes(x = col_1536, y = rev(row_1536), fill = mNG)) + 
  geom_tile()
print(Im2c)

ggsave("05_plots/01-Figure_2c.pdf", 
    height = 3.8,
    width = 6)


```


At this point, the colonies that are considered as failed crosses on the final selection plates are excluded from further calculations. These are colonies that are empty positions. 

```{r Marking small colonies}
#Change CutOFF to whatever you want.
#In my experiment, I want to remove only the non-existent colonies. If an ORF has more than 3 empty colony, the whole ORF should be deleted.

CutOFF <- 0

#Empty colonies
Empty_col <- df_colony_filtered %>%
  filter(sizeImg == 0 | is.na(ORF)) %>%
  select(plate, well_96, ORF)

Empty_col <- rename(count(Empty_col, plate, well_96, ORF), Freq = n) 

Empty_col <- Empty_col %>% filter(Freq > 2)

df_colony_filtered <- anti_join(df_colony_filtered, Empty_col, by = c("plate", "well_96"))

```

## Fluorescence normalisation by plate

The next step in the analysis is to normalize the fluorescence intensity across plates using the reference colonies pinned on each plate. The reference strains were all from the same strain (PDC1-AID*-3Myc).

Firstly, the distribution of the reference strains' fluorescence intensities are shown.

```{r Plate normalisation 1, fig.height = 14}
reference <- df_colony_filtered %>%
  filter(sample_type == "fluor_control")

```

The median reference fluorescence intensity of the plate for each fluorescence channel is determined. Then, the individual reference strains are normalised to the median of the plate's reference strains.

```{r Plate normalisation 2}
#Summarising median value
reference_plate <- reference %>%
  group_by(plate) %>%
  summarise(mNG_platerefmed = median(mNG)) %>%
  ungroup()

```

The raw fluorescence values are then divided by the local normalised reference median to obtain a plate normalised value termed "platenorm". 

```{r Plate normalisation 3}
#and calculating the plate normalised values for colonies not marked as small
df_norm <- df_colony_filtered %>%
 filter(sizeImg != 0) %>%
  group_by(plate) %>%
  ungroup %>%
  inner_join(., reference_plate, by = c("plate")) %>%
  mutate(mNG_platenorm = mNG / mNG_platerefmed)



```

The reference strains are then shown again in the following plots after normalisation

```{r Plate normalisation 4, fig.height = 14}
reference_norm <- filter(df_norm, sample_type == "fluor_control")

```


## Fluorescence and colony size normalisation, and background subtraction by ORF

After plate normalization, the colonies are normalized to the reference strains locally to remove spatial effect.

```{r Local normalisation}
reference_single <- df_norm %>%
  filter(sample_type == "fluor_control") %>%
  group_by(plate, well_96) %>%
  summarise(mNG_refnorm = median(mNG_platenorm, na.rm = T)) %>%
  ungroup()

#Re-join and normalized to local reference strain
df_norm_local <- df_norm %>%
  left_join(., reference_single, by = c("plate", "well_96")) %>%
  mutate(mNG_localnorm = mNG_platenorm / mNG_refnorm)

```

Once the fluorescence has been normalised by plate, the mean fluorescence of each ORF is calculated. The median colony size is also calculated using the centered area.

The calculated data is then combined into one table to calculate the fluorescence intensity in terms of fold over His4-AID-3Myc. From now on, the control His4-AID*-3Myc will be called wt. The resulting value is then background subtracted (assuming wt/wt is 1).

```{r background correction}
#Calculating the values for non-fluorescent colonies
wt <- df_norm_local %>%
  filter(sample_type == "control") %>%
  group_by(plate, well_96) %>% 
 # Jackknife calculations on values that are contributing to >99.5% of variance in the group
  mutate(JK.TRUE_mNG_wt = JKcalc(mNG_localnorm)) %>%
  #Filter off only the colonies that have both values considered true from jackknifing.
  filter(!(JK.TRUE_mNG_wt)) %>%
  group_by(plate, well_96) %>% 
  summarise(mNG_wt = mean(mNG_localnorm),
            mNG_wt_sd = sd(mNG_localnorm),
            areaMean_wt = mean(areaImgcentered),
            n.colonies_wt = n()) %>%
  ungroup()

#Normalising by dividing each fluorescence value by its local non-fluorescent mean
df_norm_localbgsubt <- df_norm_local %>%
  filter(sample_type != "fluor_control") %>%
  select(well, plate, well_96, ORF, gene, Fbox, sample_type, areaImgcentered, mNG_refnorm,
         mNG_platenorm, mNG_localnorm) %>%
  inner_join(., wt, by = c("plate", "well_96")) %>%
  mutate(mNG_ORFnorm = mNG_localnorm / mNG_wt,  #Normalizing for the background
         mNG_ORFnormvar = var(mNG_ORFnorm)) %>%
    group_by(plate, well_96) %>%
  #Jackknife calculations on values that are contributing to >99.5% of variance in tbe group
  mutate(JK.TRUE_mNG = JKcalc(mNG_localnorm)) %>%
  #Filter off only the colonies that have both values considered true from jackknifing.
 filter(!(JK.TRUE_mNG)) %>%
  mutate(mNG_final = mNG_ORFnorm - 1, #subtraction the background
         mNG_finalvar = var(mNG_final)) %>%
  ungroup()# %>%


```

## Processed data table for analysis

The final data frame is assembled and change in fluorescence intensities (delta_mNG) are calculated by dividing the fluorescence value of the F-Box strain with the non fluorescence value of the F-box in a log2 scale. Since this would make colonies where the final fluorescence value after degradation below zero, and consequently, no log scale, the (delta_mNG_norm) is also calculated, where the fluorescence intensities are normalized but not background corrected. The change in colony size is also calculated.

The layout of the screen was done in a way that there are 2 biological replicates already on the plate, each with 3 technical replicates. This set of code calculates the mean and the following t-test by using the individual technical replicates.

```{r Final data table construction}
#The data table below is used for plots as everything is averaged
df_delta <- df_norm_localbgsubt %>% 
  #Filter only for samples
  filter(sample_type == "sample") %>%
  select(-well_96, -n.colonies_wt) %>%
  group_by(plate, ORF, gene, Fbox) %>%
  summarise_if(is.numeric, mean, na.rm = T) %>%
  mutate(mNG_final_val = mNG_final) %>%
  mutate(log10mNG_final = log10(mNG_final),
         log2mNG_final = log2(mNG_final),
         log10mNG_norm = log10(mNG_ORFnorm),
         log2mNG_norm = log2(mNG_ORFnorm))%>%
   
 # mutate(mNG_final = ifelse(mNG_final <= 0, NA_integer_, mNG_final)) %>%
  ungroup() %>%
  #Split the table into two separate tables and combine sideways
  do({
    temp <- subset(., Fbox == F) %>% select(-Fbox)
    temp2 <- subset(., Fbox == T) %>% select(-Fbox)
    full_join(temp, temp2, by = c("plate", "ORF", "gene"),
              suffix = c("_ctrl", "_deg"))
  }) %>%
  #Calculate the delta_values for plotting
  mutate(delta_mNG = log2(mNG_final_deg / mNG_final_ctrl), #Background corrected
         delta_size = areaImgcentered_deg / areaImgcentered_ctrl,
         delta_mNG_norm = log2(mNG_ORFnorm_deg / mNG_ORFnorm_ctrl)) %>% #Normalized values by background
  drop_na(ORF)


#Now we need to join the background values to the right values

basic_ann <- read.xlsx(paste("01_annotation", "Strains positions_all.xlsx",
                            sep = "/"),
                      sheet = "library v1.1_all", startRow = 1, colNames = TRUE) %>%
  `colnames<-`(c("id", "ORF", "gene", "sgd", "plate_96", "id_96", "well_96", "row_96", "col_96"))


Step1 <- df_norm_local %>%
  filter(Fbox == T) %>%
  select(plate, well_96, ORF, gene) 

Step1 <- unique(Step1)

df_norm_localbgsubt_bkg <- df_norm_localbgsubt %>%
  filter(sample_type == "control") %>%
  select(-ORF, -gene)

df_norm_localbgsubt_bkg_2 <- Step1 %>%
  group_by(plate, well_96, ORF, gene) %>%
  left_join(., df_norm_localbgsubt_bkg, by = c("plate","well_96"))


df_delta_2 <- df_norm_localbgsubt_bkg_2 %>% 
  #Filter only for control
  select(-well_96, -n.colonies_wt, -Fbox) %>%
  mutate(log10mNG_norm = log10(mNG_ORFnorm),
         log2mNG_norm = log2(mNG_ORFnorm))%>%
  group_by(plate, ORF, gene) %>%
  summarise_if(is.numeric, mean, na.rm = T) %>% 
  ungroup() 

df_delta_3 <- full_join(df_delta, df_delta_2, by = c("plate", "ORF", "gene"))

colnames(df_delta_3)[39:50] <- paste(colnames(df_delta_3[39:50]),"bkg",sep="_")

```


```{r Add-on classification}

df_final <- df_delta_3
df_final <- df_final[!duplicated(df_final$ORF),]

save(df_final, file = "data_df_final.rda")

```

### Significance test calculations

To determine if the changes are significant, a t-test is carried out and corrected for multiple testing using the Benjamini & Hochberg (BH) correction.

```{r Performing ttest calculations}
#T-test calculations are done here.
#The tables are subset for each channel for the calculation individually.
#Since some rows may have green channel values but not red channel values, and vice versa.
#Naturally, if either is NA, ratio will be NA.

significant <- df_norm_localbgsubt %>%
  do({
    
    tempA <- .

    tempC <- tempA %>% #drop_na(mNG_final) %>%
      group_by(ORF, Fbox) %>%
      filter(n() > 1) %>% #Check that it has at least 1 observation
      group_by(ORF) %>%
      filter(length(unique(Fbox)) == 2) %>% #Check that each ORF has both conditions
      summarise(pvaluemNG = t.test(mNG_final~Fbox)$p.value) %>%
      ungroup()
    

    tempE <- tempA %>% #drop_na(areaImgcentered) %>%
      group_by(ORF, Fbox) %>%
      filter(n() > 1) %>% #Check that each OE type has at least 1 observation
      group_by(ORF) %>%
      filter(length(unique(Fbox)) == 2) %>% #Check that each ORF has both conditions
      summarise(pvaluesize = t.test(areaImgcentered~Fbox)$p.value) %>%
      ungroup()
    
    tempF <- tempA %>%
      group_by(ORF) %>%
      count() %>%
      left_join(., tempC, by = c("ORF"),
                       suffix = c("", "_int")) %>%

      left_join(., tempE, by = c("ORF"),
                       suffix = c("", "_size"))
    
    tempF
    
  }) %>%
  ungroup() %>%
  #The BH adjustments are done per carbon source plate, per incubation temp, per replicate, and per day.
#  group_by(IncTemp, Rep, carbonsource, Day, Notes) %>%
  mutate(BHmNG = p.adjust(pvaluemNG, method = "BH"),
         BHsize = p.adjust(pvaluesize, method = "BH")) %>%
#  ungroup() %>%
  #Remove rows where there are no p-values for the variable of mNG.
  #The size variable is not considered since it will always have a colony size.
  unite("check", 3, remove = F, na.rm = T) %>%
  filter(check != "") %>% select(-n, -check) %>%
  left_join(., df_final, by = c("ORF"))
significant <- significant[!duplicated(significant$ORF),]


save(significant, file = "data_significant.rda")

```


# Data Analysis

```{r Loading analysed Data}

load("data_df_final.rda")
load("data_significant.rda")

```


# Setup - Size analysis using SGATools

Here, several functions and R packages that are necessary for the analysis are loaded.

```{r Setup 1, echo = FALSE}
#setwd("C:/Users/gameiroe/Desktop/Livrary_v1")

#knitr::opts_chunk$set(echo = TRUE, fig.width = 24, fig.height = 12,
#                      warning = FALSE, message = FALSE, error = TRUE)

```

```{r Setup 2 Functions}
# Function for loading and installing packages obtained from 
# https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them
# and adapted to current use case.
# @params ...: Package names as strings
LoadPackages <- function(...){
  
  libs <- unlist(list(...))
  req <- unlist(lapply(libs, require, character.only = TRUE))
  need <- libs[req == FALSE]
  n <- length(need)
  
  if(n > 0) {
    libsmsg <- if(n > 2) {
      paste(paste(need[1:(n-1)], collapse = ", "), ",", sep = "")
      } else { need[1] }
    
    print(libsmsg)
    
    if(n > 1) {
      libsmsg <- paste(libsmsg, " and ", need[n], sep = "")
      }
    
    libsmsg <- paste("The following packages could not be found: ", libsmsg,
                     "\n\r\n\rInstall missing packages?", collapse = "")
    
    if(winDialog(type = c("yesno"), libsmsg) == "YES") {
      install.packages(need)
      lapply(need, require, character.only = TRUE)
    }
  }
}

# Function for plotting heatmaps of colony sizes on a plate
# @params data_table: data frame containing plate number, row, column, and 
#   colony size data for plotting.
# @params plate_number: character or number indicating the plate to be plotted.
# @params fill_col: name of column containing colony size data
# @params source_type: string describing the colony size type (e.g. raw,
#   median-transformed, normalized)
# @params lowerlimit: number indicating the bottom of the plotted data
# @params midpoint: number indicating the median of the plotted data
# @params upperlimit: number indicating the ceiling of the plotted data
# @params breakpoints: number indicating the breaks for the scale
# @params na_colour: string indicating colour for NA values
heatplot <- function(data_table, plate_number,
                     fill_col = "size", source_type = "Raw",
                     lowerlimit = 0, midpoint = 500, upperlimit = 1000,
                     breakpoints = 200, na_colour = "white"){
  
  ## Install and load the package needed
  if (!requireNamespace("tidyverse", quietly = TRUE)) {
    install.packages("tidyverse")
  
  require(tidyverse)
  }
  
  ## Plotting the data using geom_tile from ggplot2
  ggplot(data = data_table %>%
           filter(plate_1536 == plate_number),
         aes(as.factor(col), reorder(as.factor(row), -row))) +
    geom_tile(aes_string(fill = fill_col)) +
    scale_fill_gradientn(name = paste0("Size (", source_type, ")"),
                         na.value = na_colour,
                         colours = c("lightblue1", "cyan", "black", "yellow3", "yellow"),
                         limits = c(lowerlimit, upperlimit),
                         breaks = seq(lowerlimit, upperlimit, breakpoints)
                         ) +
    coord_fixed(0.8) +
    theme_light(base_size = 26) +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank()) +
    scale_y_discrete(limits = as.character(rev(seq(5, 28, 1))),
                     breaks = as.character(seq(5, 28, 1))) +
    scale_x_discrete(limits = as.character((seq(5, 44, 1))),
                     breaks = as.character(seq(5, 44, 1))) +
    labs(x = "Column", y = "Row",
         subtitle = paste("Plate", plate_number))

}

# Function needed for jackknife function (Taken from SGA Tools)
# @params x: a numeric vector
theta <- function(x){ sd(x, na.rm = T) }

# Function for GO Terms enrichment analysis
# @params background: data frame containing list of all ORFs in a screen with
#   GO Term information.
# @params input: data frame containing list of candidate ORFs from a screen with
#   GO Term information.
# @params BGlist: data frame with a column containing all ORFs (has to be same
#   list of ORFs with background)
# @params Inputlist: data frame with a column containing all candidate ORFs (has
#   to be same list of ORFs with input)
# @params error_table: empty data frame for GO terms that encounter a problem
EnrichGOTERMS <- function(background, input, BGlist, Inputlist){
  
  #### Preparation ####
    if (!requireNamespace("tidyverse", quietly = TRUE)) {
    install.packages("tidyverse")
  
  require(tidyverse)
  }
  
  ## List out all possible unique GO term identifiers as a list from the hits
  # GO parent term identifier is used to calculate p-values
  All_Possible_Identifiers <- unique(input$Gene.goAnnotation.ontologyTerm.parents.identifier)
  
  ## Count what is the number of unique GO term identifiers.
  unique_length <- length(All_Possible_Identifiers)
  
  ## Create a table to hold the calculated p-values from the previous hits table
  # P-value of each GO term is needed, so group by GO Terms
  newtable <- input %>%
    group_by(Gene.goAnnotation.ontologyTerm.parents.identifier,
             Gene.goAnnotation.ontologyTerm.parents.name,
             Gene.goAnnotation.ontologyTerm.parents.namespace) %>%
    # Name summary column as 'pval' which will be set to NA
    summarise(pval = n()) %>%
    ungroup %>%
    # Counts of number of ORFs in the hits and in the population are also needed
    mutate(pval = NA,
           Hits = NA,
           Population = NA)
  
  ## Enrichment calculation using hyper geometric distribution
  # A loop is used to go through all GO terms within the data set and perform
  #   the calculations needed.
  # 'All_Possible_Identifiers' is used here as it is from the background data
  #   and should encompass all the terms for the hits as well.
  for(x in 1:unique_length) {
    
    ## Print message to show which GO ID is being checked
    print(paste0(x, "/", unique_length,
                 " Analyzing GO ID: ", All_Possible_Identifiers[x],
                 " - GO Term: ", unique((background %>%
           filter(Gene.goAnnotation.ontologyTerm.parents.identifier ==
                    All_Possible_Identifiers[x])
           )$Gene.goAnnotation.ontologyTerm.parents.name)))
    
    ## Perform calculations
    # Calculate all the needed data/numbers for phyper function
    # The number of ORFs must be unique and not repeated for each GO Term
    
    hits_anno <- length(unique(
      (input %>%
         filter(Gene.goAnnotation.ontologyTerm.parents.identifier ==
                  All_Possible_Identifiers[x])
       )$Gene.secondaryIdentifier))
    
    pop_anno <- length(unique(
      (background %>%
         filter(Gene.goAnnotation.ontologyTerm.parents.identifier ==
                  All_Possible_Identifiers[x])
       )$Gene.secondaryIdentifier))
    
    hits_length <- nrow(Inputlist)
    
    pop_length <- nrow(BGlist)
    
    # P-value is calculated using phyper function as annotated on the
    # YeastMine API documentation
    pvalue <- phyper(q = hits_anno-1, m = pop_anno,
                     n = pop_length-pop_anno, k = hits_length,
                     lower.tail = FALSE)
  
    ## Store the calculated p-value and ORF counts
    newtable$pval[newtable$Gene.goAnnotation.ontologyTerm.parents.identifier ==
                    All_Possible_Identifiers[x]] <- pvalue
    
    newtable$Hits[newtable$Gene.goAnnotation.ontologyTerm.parents.identifier ==
                    All_Possible_Identifiers[x]] <- hits_anno
    
    newtable$Population[newtable$Gene.goAnnotation.ontologyTerm.parents.identifier ==
                          All_Possible_Identifiers[x]] <- pop_anno
    }
  
  return(newtable)
  
}

```

```{r Setup 3 Directory, Packages n Datasets}
# Getting the main working directory
PrimaryDirectory = getwd()

# Loading the required R packages
LoadPackages("tidyverse", "kableExtra", "gdata", "openxlsx", "gridExtra", "grid", "patchwork", "VennDiagram", "EnvStats", "janitor")

# Setting the ggplot theme
theme_set(theme_light(base_size = 26))

# Listing the rows for a 1536 array
alphabet1536 <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L",
                  "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X",
                  "Y", "Z", "AA", "AB", "AC", "AD", "AE", "AF")

alphabet1536_rev <- 1:length(alphabet1536)
names(alphabet1536_rev) <- alphabet1536

# Import an Excel file containing the coordinates of the border colonies
BorderColonies <- read.xlsx("06_datasets/BorderColoniesList.xlsx",
                            startRow = 1, colNames = TRUE, sheet = "Sheet1")

```


## SGA Tools

Additionally, SGA tools from https://github.com/boonelab/sgatools/blob/master/public/SGAtools/SGAtools.R are loaded for the normalizing functions that will be used in this analysis. The functions related to normalization were modified as described below.

```{r Setup SGA tools}
####################################################################################
# SGATools v1.2.1
# Tools for image processing, normalizing and scoring Synthetic Genetic Array screens.
#
# This software is in the public domain, furnished "as is", without technical
# support, and with no warranty, express or implied, as to its usefulness for
# any purpose.
#
# Author:         Omar Wagih
# Licence:        Academic Free Licence v3.0
# Language:       English (CA)
# Last modified:  19/11/12
# 
# Modification History 
# --------------------
# Dated  Version		Who		Description
# ----------------------------------------------------------
# 2020-05  1.1     JF    Edited normalizeSGA function as described below:
#     1)  Removed parameters 'keep.large', "intermediate.data", "linkage.file",
#         "linkage.genes". Creates issues because the parameters are not used.
#     2)  Changed "max.colony.size" param to 3*overall.plate.median
#     3)  Added new parameter "field.to.normalize" because we have different entry
#         points and the column is called differently to the default in the code
#     4)  Changed num.rows and num.cols to get the information from the global
#         environment instead of the data table.
#     5)  Removed "rdbl", "cdbl", and "spots" because of difference in the data
#         table setup
#     6)  Removed (F1) Linkage effect filter
#     7)  For (N1) plate normalization, changed 'colonysize' to parameter called
#         "field.to.normalize"
#     8)  Removed (F2) Big replicates filter
#     9)  Removed (F3) Jackknife filter. This is done later in the analysis at
#         a different point in the analysis.
#     10) Removed setting data to "ncolonysize", capping normalized colony size,
#         and adding status codes steps.
#     11) Removed all items after (N4) Plate Normalization 2, and immediately
#         return plate data after this step.
# 2020-05  1.1     JF    Edited spatialNormalization function as described below:
#     1)  Changed num.rows and num.cols to get the information from the global
#         environment instead of the data table.
#     2)  Removed line where ignore indices is set to NA:
#         ("after.ignore[ignore.ind] = NA")
# 2020-05  1.1     JF    Edited rowcolNormalization function as described below:
#     1)  Removed line where ignore indices is set to NA:
#         ("after.ignore[ignore.ind] = NA")
#     2)  Changed num.rows and num.cols to get the information from the global
#         environment instead of the data table.
# 2020-05  1.1     JF    Edited rowcolNormalizationHelper function as described below:
#     1)  Modified the section to determine window size to be used in lowess
#     2)  Simplified the lowess calculation section
# 2020-05  1.1     JF    Edited fgaussian function as below:
#     1)  Changed the seq helper function to SGAseq because of the change in
#         name of the function
# 2020-05  1.1     JF    Edited seq function as described below:
#     1)  Renamed the function to SGAseq as it was interfering with the base
#         seq() function
# 2020-10  1.2     JF    Packages loading modified
#     1)  Use LoadPackages() to load required libraries
# 2020-10  1.2.1     EG    Normalized size values changed
#     1)  Median and MAD calculated with the colonies between 40-80% of the plate size 
#     2)  The row and col normalization was deleted due to overnormalization of
#         colonies size.
####################################################################################

# Load required libraries 
# "logging" is for log file
# "stringr" is for regex matching functions
# "bootstrap" is for jackknife function
LoadPackages("logging", "stringr", "bootstrap")


addHandler(writeToConsole)

# returns string w/o leading whitespace
trim.leading <- function (x)  sub("^\\s+", "", x)

# returns string w/o trailing whitespace
trim.trailing <- function (x) sub("\\s+$", "", x)

# returns string w/o leading or trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

SGATOOLS_VERSION = '1.2'
####################################################################################
# Reading section
readSGA <- function(file.paths, file.names=basename(file.paths), ad.paths=NA, replicates=4){
  
  # Length of vectors
  n1 = length(file.paths)
  n2 = length(file.names)
  
  loginfo('Reading input files')#
  
  sga.data.list = lapply(file.paths, function(file.path){
    # Index of the file path
    file.ind = which(file.paths == file.path)
    
    # Name of file 
    file.name = file.names[file.ind]
    
    loginfo('Reading input file (%d/%d) path = %s', file.ind, n1, file.path)
    
    # Read all lines first line, except @ and # symbols
    file.lines = readLines(file.path) 
    comment.meta = file.lines[grepl('@|#|[(]', file.lines)]
    file.lines = file.lines[!grepl('@|#|[(]', file.lines)]
    
    # If first line is Colony Project Data File, skip the first 13 lines 
    if(grepl('Colony Project Data File', file.lines[1], ignore.case=T)){
      loginfo('* Detected boone-lab format, skipping first 13 lines')
      file.lines = file.lines[-(1:13)]
      file.lines = sapply(file.lines, trim)
      file.lines = gsub(pattern='\\s+',replacement='\t',file.lines)
    }
    
    # Read in the data: we only care about the first 3 columns
    sga.data = read.delim(textConnection(file.lines), stringsAsFactors=F, header=F)[1:3]
    names(sga.data) = c('V1', 'V2', 'V3')
    sga.data = sga.data[with(sga.data, order(V1, V2)), ]
    
    loginfo('* Done reading')
    
    # Find number of rows and columns
    num.rows = max(sga.data[[1]])
    num.cols = max(sga.data[[2]])
    
    loginfo('* Column classes = %s', sapply(sga.data, class))
    loginfo('* Number rows = %d', num.rows)
    loginfo('* Number cols = %d', num.cols)
    loginfo('* Data dimension = %s', dim(sga.data))
    
    file.name.metadata = fileNameMetadata(file.name)
    
    loginfo('* Obtaining file name metadata: valid = %s', file.name.metadata$is.valid)
    
    # Add plate id as file name
    sga.data[[4]] = file.name.metadata$filename
    
    rdbl = ceiling(sga.data[[1]]/sqrt(replicates))
    cdbl = ceiling(sga.data[[2]]/sqrt(replicates))
    
    # Default query - index of file
    sga.data[[5]] = as.character(file.ind)
    
    # Do we have valid meta data in the file name
    if(file.name.metadata$is.valid){
      # Update query
      sga.data[[5]] = file.name.metadata$query
    }
    
    # Default arrays as numbers - each replicate has a unique number 
    array.vals = ((cdbl - 1)* (num.rows/sqrt(replicates))) + rdbl
    
    # Map to array definition files
    sga.data[,6:7] = mapArrayDefinition(file.name.metadata, array.vals, 
                                       rdbl, cdbl, ad.paths)
    
    # Set normalized colony size / score / KVP to NA 
    sga.data[,8:10] = NA
    
    # Add comment / meta data lines
    comment(sga.data) = comment.meta
    
    # Add other attributes
    attr(sga.data, 'file.name.metadata') = file.name.metadata
    attr(sga.data, 'num.rows') = num.rows
    attr(sga.data, 'num.cols') = num.cols
    
    # Set column names of data
    names(sga.data) = c('row', 'col', 'colonysize',
                        'plateid', 'query','array', 'array_annot',
                        'ncolonysize', 'score', 'kvp'  )
    sga.data
  })
  
  loginfo('Done reading all files')
  loginfo('-----------------------------------------------')
  return(sga.data.list)
}

mapArrayDefinition <- function(file.name.metadata, array.vals, rdbl, cdbl, ad.paths){
  # If we have array definition files - i.e they are not all NA
  if(! all(is.na(ad.paths))){
    loginfo('* Mapping array definition: number of array definition files = %d', length(ad.paths))
    
    # Get file names of paths
    ad.basenames = basename(ad.paths)
    
    ap.ids = str_extract(tolower(ad.paths), 'plate\\d+')
    if (is.na(ap.ids)) {
        loginfo('* Cherry picker plate name malformed... assume only one plate is available')
        ap.ids = c(file.name.metadata$arrayplateid)
    } else {
        ap.ids = as.numeric(str_extract(ap.ids, '\\d+'))
    }
    
    loginfo('* Mapping array definition: IDs of array plate files = %s', ap.ids)
    
    # Assume no valid array plate id, use the first one we have
    ind = 1
    
    if(file.name.metadata$is.valid){
      # If our file has a valid array plate id, match it to its array definition file path
      ind = which(file.name.metadata$arrayplateid == ap.ids)[1]
    }
    
    if(!is.na(ind)){
      # Read in corresponding array definition file - handle 5 lines?
      ad.data = read.table(ad.paths[ind], sep='\t', skip=5, header=F, stringsAsFactors=F)
      names(ad.data)[1:3] = c('c', 'r', 'Gene')
      
      m=data.frame(r=rdbl, c=cdbl)
      i = apply(m, 1, function(x){
        intersect(which(x[1] == ad.data$r), which(x[2] == ad.data$c))[1]
      })
      good.ind = !is.na(i)
      array.vals[good.ind] = ad.data$Gene[i][good.ind]
    }
    
  }else{
    loginfo('* Not mapping array definition: no array definition files')
  }
  
  ret = as.character(array.vals)
  
  orfs = unlist( lapply(strsplit(ret, '_'), function(i) i[1]) )
  annots = unlist( lapply(strsplit(ret, '_'), function(i) i[2]) )
  
  if (all(is.na(annots))) {
    annots = orfs
  }
  
  return(c(orfs, annots))
}

# Get metadata encoded in file name
# Format should be: username_query_arrayplateid...
# @param file.name: character file name
# @return ret: list of all meta data encoded in file name
fileNameMetadata <- function(file.name){
  split.pat =  '_|\\.'
  
  sp = strsplit(file.name,split.pat)[[1]]
  
  # Regular expression query of control screens
  ctrl.pat = 'wt|ctrl'
  # Regular expression for digit
  digit.pat = '^\\d+$'
  
  ret = NULL
  
  # Set file name - replace any spaces by a hyphen
  ret$filename = sub('\\s', '-', file.name)
  
  # Set user name - replace any spaces with a hyphen
  ret$username = sub('\\s', '-', sp[1])
  
  # Set query name
  ret$query = sub('\\s', '-', sp[3])
  
  # Set is.control
  ret$is.control = grepl(ctrl.pat, sp[2], ignore.case=T)
  
  # Set array plate id
  if(grepl(digit.pat, sp[4])){
    # Valid array plate id
    ret$arrayplateid = as.numeric(sp[4])
  }else{
    # Invalid array plate id
    ret$arrayplateid = NA
  }
  
  # Set is valid only if we have no NAs 
  ret$is.valid = !any(is.na(c(ret$query, ret$arrayplateid)))
  
  return(ret)
}

####################################################################################
# Normalization/scoring section

# Normalize a plate
# @params overall.plate.median: value of overall plate median from large-scale experiments
# @params max.colony.size: computed from plate median, used to filter large colonies
# @return data frame: normalized data frame
# The normalizeSGA function was edited from the original
normalizeSGA <- function(
        plate.data,
        overall.plate.median = 510,
        max.colony.size = 3*overall.plate.median, # Changed this to 3 to keep everything. Threshold filter is done later.
        field.to.normalize = 'size'
		){
  
  loginfo('Normalizing plate: overall.plate.median = %d, max.colony.size = %d, nROW = %d, nCOL = %d', 
          overall.plate.median, max.colony.size, nROW, nCOL)

  # Edited so it is filled in at the start of the function with nROW and nCOL from the global environment.
  num.rows = nROW
  num.cols = nCOL

  # Ignored rows (not used as we are not filtering; kept for ease of the rest of the code)
  ignore.ind = rep(FALSE, nrow(plate.data))
  names(ignore.ind) = NA
 
  ########## (N1) Plate normalization ##########
  plate.data$pnorm = plateNormalization(plate.data, field.to.normalize, overall.plate.median)

  ########## (N2) Spatial normalization ##########
  # The spatialNormalization function was edited from the original
  plate.data$snorm = spatialNormalization(plate.data, 'pnorm', ignore.ind)

  ########## (N3) Row column effect normalization ##########
  # The rowcolNormalization function was edited from the original
 # plate.data$rcnorm = rowcolNormalization(plate.data, 'snorm', ignore.ind)

  ########## (N4) Plate normalization 2 ##########
  plate.data$pnorm2 = plateNormalization(plate.data, 'snorm', overall.plate.median)
  
  # The last section on normalizing to the overall plate median is removed.
  
  return(plate.data)
}

# Score plates
# @params plate.data.list: list of data frames (plate data)
# @params scoring.function: 1 for Cij-CiCj, 2 for Cij/CiCj
# @return list: list of plate files, scored
scoreSGA <- function(plate.data.list, scoring.function=1){
  
  # Merge list info
  merged.dat = do.call('rbind', plate.data.list)
  
  metadata.table = lapply(plate.data.list, function(plate.data){
    d = attr(plate.data, 'file.name.metadata')
    as.data.frame(d)
  })
  metadata.table = do.call('rbind', metadata.table)[,4:6]
  
  # If we dont have any control/dm plates, we cant do scoring
  if(sum(metadata.table$is.control) < 1 | sum(!metadata.table$is.control) < 1)
    return(plate.data.list)
  
  # Do scoring for different array plate ids separatley 
  for(arrayplateid in unique(metadata.table$arrayplateid)){
    
    # Check if this array plate id has controls. if not, dont do anything
    is.ctrl = metadata.table$is.control & metadata.table$arrayplateid == arrayplateid
    is.dm = !metadata.table$is.control & metadata.table$arrayplateid == arrayplateid
    
    if(sum(is.ctrl) < 1 | sum(is.dm) < 1)
      next
    
    merged.dat.ctrl = do.call('rbind', plate.data.list[is.ctrl])
    merged.dat.dm = do.call('rbind', plate.data.list[is.dm])
    
    # We have at least one query, proceed to score
    # Get single mutant fitness of arrays (non control plates) - computed as median of the plate
    querys = unique(merged.dat.dm$query)
    query.smf = sapply(querys, function(curr.query){
      median( merged.dat.dm$ncolonysize[merged.dat.dm$query == curr.query] , na.rm=T)
    })
    
    # Get array smf from control plates
    arrays = unique(merged.dat.ctrl$array_annot)
    array.smf = sapply(arrays, function(curr.array){
      median( merged.dat.ctrl$ncolonysize[ merged.dat.ctrl$array_annot == curr.array ], na.rm=T )
    })
    
    # Use default overall median or median from plates?
    # Default overall median
    overall.median = 510
    # Get 60% middle median - R automatically removes NA values
    vals = sort(merged.dat$ncolonysize)
    length = length(vals)
    lower = 0.4
    upper = 0.8
    middle.median = median( vals[round(lower*length):round(upper*length)], na.rm = T)
    
    # Do the scoring
    plate.data.list[is.dm] = lapply(plate.data.list[is.dm], function(plate.data){
      # Single mutant fitnesses
      q.smf = query.smf[plate.data$query] / middle.median
      a.smf = array.smf[plate.data$array_annot] / middle.median
      # Double mutant fitness
      dm = plate.data$ncolonysize / middle.median
      
      # Score accourding to scoring function
      if(scoring.function == 1){
        plate.data$score = dm - (q.smf * a.smf)
      }else if(scoring.function == 2){
        plate.data$score = dm / (q.smf * a.smf)
      }
      
      plate.data$ctrlncolonysize = plate.data.list[is.ctrl][[1]]$ncolonysize;
      
      plate.data
    })
    
  }# End array plate id loop
  
  # Done scoring, return the data
  return(plate.data.list)
}

# Merges names of logical vectors, collapsing using a comma
# @param a: logical vector 1
# @param b: logical vector 2
# @return logical: logical vector 1 and 2 merged
mergeLogicalNames <- function(a, b){
  ret = a | b
  sp1 = strsplit( names(a), ',')
  sp2 = strsplit( names(b), ',')
  
  new.nm = sapply(1:length(sp1), function(i){
    u = as.character(union(sp1[[i]], sp2[[i]]))
    u = u[!is.na(u) & !grepl('NA', u)]
    paste(u, collapse=',')
  })
  names(ret) = new.nm
  return(ret)
}

#  Key-value pair methods
# Convert character vector of kvps to a data frame
# @param kvps.string: character verctor of kvps  
# @return data frame: columns are keys, rows are different kvps
kvpsAsDataFrame <- function(kvps.string){
  # Get list of dataframes
  list.df = lapply(kvps.string, function(kvp.string){
    if(!is.na(kvp.string)){
      df = as.data.frame(kvpAsMap(kvp.string))
      t(df)
    }
  })
  # Remove NULL
  list.df = list.df[!sapply(list.df, is.null)]
  
  # If nothing produced we return an empty data frame
  if(length(list.df) == 0) return(as.data.frame(matrix(NA,0,0)))
  
  #Merge keys/value data frames
  merged.df = Reduce(function(...) merge(..., all=T), list.df)
  
  # Rename so everything is uppercase
  names(merged.df) = toupper(names(merged.df))
  return(merged.df)
}

# Convert key-value pair string to key-value map 
# @param kvp.string: key value pair as character 
# @return list: named vector
kvpAsMap <- function(kvp.string){
  # As a vector
  split = strsplit(kvp.string, '\\{|\\}|,\\s*')[[1]]
  split = split[split != ""]
  
  # Create the map
  df = as.data.frame(strsplit(split, '='), stringsAsFactors=F)
  map = as.character(df[2,])
  names(map) = df[1,]
  
  return(map)
}

# Convert key-value pair map to a character string 
# @param kvp.map: key value pair as map 
# @return list: character representation
kvpMapAsString <- function(kvp.map){
  kv = sapply(names(kvp.map), function(key){
    paste0(key, '=', kvp.map[[key]])
  })
  return(paste0('', paste0(kv, collapse=','), ''))
}

# Linkage filter: check if query and array are within close proximity on the same chromosome
# @param plate.data: SGA formatted data frame
# @param linkage.cutoff: in KB, If witin this value of eachother on same chromosome they will be ignored
# @return linkage.ignore: logical array with TRUE for rows to ignore. Status code as name
linkageFilter <- function(plate.data, linkage.cutoff=200, linkage.file='', linkage.genes=''){
  
  loginfo('# Applying linkage filter, linkage.cutoff = %d', linkage.cutoff)
  
  loginfo('Linkage file is at: %s', linkage.file)
  status.code = 'LK'
  
  # Load linkage files named: chromosome_coordinates.Rdata(R.data?)
  if(file.exists(linkage.file)){
    loginfo('Loading chromosome coordinates file')
    load('data/chrom_coordinates.Rdata')
  }else{
    loginfo('Chromosome coordinates file does not exist, returning empty data frame')
    chrom_coordinates = as.data.frame(matrix(NA, 0, 4))
  }
  
  if(linkage.cutoff < 0){
    loginfo('Skipping linkage correction...')
    linked = rep(FALSE, nrow(plate.data))
    names(linked) = NA
    return(linked)
  }
  
  
  mid.map = apply(chrom_coordinates[,3:4], 1, mean)
  names(mid.map) = chrom_coordinates[[1]]
  
  chr.map = chrom_coordinates[[2]]
  names(chr.map) = chrom_coordinates[[1]]
  
  linkage.genes = unique(c(plate.data$query, linkage.genes))
  loginfo('# Linkage genes including query = %s', paste0(linkage.genes, collapse=', '))
  linkage.genes = linkage.genes[ linkage.genes %in% chrom_coordinates[[1]] ]
  loginfo('# Linkage genes found in coords table = %s', paste0(linkage.genes, collapse=', '))
  
  # Get indicies for which row:query/array on same chromsome and within < cutoff
  #ind = plate.data$array %in% chrom_coordinates[[1]] 
  ar = plate.data$array
  
  linked = sapply(linkage.genes, function(g){
    (chr.map[g] == chr.map[ar]) & (abs( mid.map[g] - mid.map[ar] ) < (linkage.cutoff * 1e3))
  })
  
  linked = apply(linked, 1, any)
  linked[is.na(linked)] = FALSE
  
#   linked = sapply(plate.data$array, function(ar){
#     if(length(linkage.genes) == 0 | ! ar %in% chrom_coordinates[[1]] | linkage.genes[1] == ''){
#       FALSE
#     }else{
#       t = sapply(linkage.genes, function(g){
#         (chr.map[g] == chr.map[ar]) & (abs( mid.map[g] - mid.map[ar] ) < (linkage.cutoff * 1e3))
#       })
#       if(any(is.na(t) | is.null(t))){
#         FALSE
#       }else{
#         any(t)
#       }
#     }
#   })
  
  # Set status code
  names(linked) = NA
  ind = which(linked)
  names(linked)[ind] = rep(status.code, length(ind))
  
  loginfo('Linkage filter applied, total ignored = %d',sum(linked))
  return(linked)
}

# Plate normalization: brings all plates to one same scale
# @param plate.data: SGA formatted data frame
# @param field.to.normalize: name of the column in the data to normalize
# @return: vector of normalized values
plateNormalization <- function(plate.data, field.to.normalize, default.overall.median) {
  
  loginfo('# Normalizing for plate effect, default.overall.median = %d', default.overall.median)
  #Used to get median of center 60% of colonies (change if needed)
  lower = 0.4
  upper = 0.8
  
  # Get and sort our data to be plate normalized
  vals = sort(plate.data[[field.to.normalize]])
  vals.length = length(vals)
  
  # If we have insufficient data - return NAs
  if(length(vals) < 10){
    loginfo('Insufficient data for plate normalization, returning')
    return( rep(NA, nrow(plate.data)) )
  }
  
  # We have sufficient data - get median of center 60% of colonies
  plate.median = median(vals[round(lower*vals.length) : round(upper*vals.length)], na.rm = TRUE)
  
  if (plate.median == 0) {
    loginfo('Median is 0, taking median of all')
    plate.median = mean(vals, na.rm = TRUE)
    loginfo(paste('New median is', plate.median))
  }
  
  # Store the final result computed using all data in result array
  normalized = plate.data[[field.to.normalize]] * (default.overall.median / plate.median)
  
  loginfo('Done plate normalization')
  
  # Return final result
  return(normalized)
}

# Spatial normalization: normalizes any gradient effect on the plate via median smoothing
# @param plate.data: SGA formatted data frame
# @param field.to.normalize: name of the column in the data to normalize
# @param ignore.ind: logical for any rows to be ignored 
# @return: vector of normalized values
# The spatialNormalization function was edited from the original
spatialNormalization <- function(plate.data, field.to.normalize, ignore.ind) {

  loginfo('# Normalizing for spatial effect')

  # Edited so it is filled in at the start of the function with nROW and nCOL from the global environment.
  num.rows = nROW
  num.cols = nCOL
  
  # Get gaussian/average filters
  gaussian.filt = fgaussian(7, 2) # smaller number, better resolution between neighbouring colonies
  average.filt = faverage(9)
  
  # Data to be normalized before ignored
  before.ignore = plate.data[[field.to.normalize]]
  
  # Data to be normalized after ignored (used in the analysis)
  after.ignore = before.ignore
  
  # Construct plate matrix
  plate.mat = matrix(NA, num.rows, num.cols)
  rc.mat = as.matrix(plate.data[,1:2])
  plate.mat[rc.mat] = after.ignore
  
  # Fill NA with a placeholder (mean of all colonies) 
  t = plate.mat
  ind.na = which(is.na(t))
  t[ind.na] = mean(plate.mat, na.rm = TRUE)
  
  # Fill in NA with smoothed version of neighbors using gaussian blur
  filt.g = applyfilter(t, gaussian.filt)
  t[ind.na] = filt.g[ind.na]
  
  # Apply median/average filters
  # Padding type "replicate" is to copy nearest cells and this way it makes the border colonies have more large sized colonies on an outer "fake" border to normalise it.
  filtered = medianfilter2d(t, 7, padding_type = 'replicate')
  filtered = applyfilter(filtered, average.filt, 'replicate')
  
  # Subtract the mean of the filtered data from the filtered data
  f = filtered / mean(filtered)
  
  # Subtract filtered - mean from  
  before.ignore = before.ignore / f[rc.mat]
  
  return(before.ignore)
}

# Row column effect normalization
# @param plate.data: SGA formatted data frame
# @param field.to.normalize: name of the column in the data to normalize
# @param ignore.ind: logical for any rows to be ignored 
# @return: vector of normalized values
# The rowcolNormalization function was edited from the original
rowcolNormalization <- function(plate.data, field.to.normalize, ignore.ind) {
  
  loginfo('# Normalizing for row column effect')

  # Data before rows ignored
  data.before.ignore = plate.data[[field.to.normalize]]
  
  # Ignore these rows
  data.after.ignore = data.before.ignore
  
  # Edited so it is filled in at the start of the function with nROW and nCOL from the global environment.
  num.rows = nROW
  num.cols = nCOL
  
  # Smooth across columns
  # The rowcolNormalizationHelper function was edited from the original
  col.data = plate.data$col
  norm.col.effect = rowcolNormalizationHelper(col.data, data.after.ignore, num.rows, num.cols)
  
  # Smooth across rows
  # The rowcolNormalizationHelper function was edited from the original
  norm.col.effect[ignore.ind] = NA
  row.data = plate.data$row
  norm.rowcol.effect = rowcolNormalizationHelper(row.data, norm.col.effect, num.rows, num.cols)
  
  return(norm.rowcol.effect)
}

# Helper for row/column effect normalization to avoid redundancy 
# @param rowcol.data: values of the row/col column in the plate data frame
# @param colony.size.data: values of the colony sizes being normalized 
# @return: vector of normalized values
# The rowcolNormalizationHelper function was edited from the original
rowcolNormalizationHelper <- function(rowcol.data, colony.size.data, num.rows, num.cols){
  
  ind.na = is.na(colony.size.data)
  
  # Sort values with index return
  vals.sorted = sort(rowcol.data[!ind.na], index.return = TRUE)
  ind.sorted = vals.sorted[[2]]
  vals.sorted = vals.sorted[[1]]
  
  # Original code start
  # Window size to be used in lowess smoothing - currently not using it
  #span = sum(vals.sorted <= 6) / (num.rows*num.cols)
  
  #if(span>0 & length(span) > 0){
  #lowess_smoothed = lowess(rowcol.data[!ind.na][ind.sorted], colony.size.data[!ind.na][ind.sorted], f=0.09, iter=5)
  #}else{
  #  lowess_smoothed = list(y=colony.size.data[!ind.na][ind.sorted])
  #}
  # Original code end
  
  #Modified
  lowess_smoothed = lowess(rowcol.data[!ind.na][ind.sorted], colony.size.data[!ind.na][ind.sorted], f = 0.09, iter = 5)
  
  #      pdf(sprintf('~/Desktop/lowess_%s_%s.pdf', max(rowcol.data, na.rm=T), i), width=18, height=18)
  #      x = rowcol.data[!ind.na][ind.sorted]
  #      y = colony.size.data[!ind.na][ind.sorted]
  #      plot(x,y)
  #      lines(lowess_smoothed, col='green')
  #      lx = lowess_smoothed$x
  
  # We only care about Y values (colony size)
  lowess_smoothed = lowess_smoothed[['y']]
  
  tmp = lowess_smoothed / mean(lowess_smoothed)
  tmp[is.nan(tmp)] = 1
  colony.size.data[!ind.na][ind.sorted] = colony.size.data[!ind.na][ind.sorted] / tmp;
  colony.size.data[is.infinite(colony.size.data)] = 0
  
  #    lines(x=lx, tmp, col='red')
  #    points(x=lx, colony.size.data[!ind.na][ind.sorted] / tmp, col='blue', pch=3)
  #    dev.off()
  
  # Fill ignored NA values
  ind.uniq.rc = which(duplicated(rowcol.data))
  rc.to.smoothed = lowess_smoothed[ind.uniq.rc]
  names(rc.to.smoothed) = rowcol.data[ind.uniq.rc]
  
  ind.na = which(ind.na)
  
  na.smoothed = sapply(ind.na, function(i){
    i.rc = as.character(rowcol.data[i]) 
    
    i.smoothed = rc.to.smoothed[i.rc]
    
    i.smoothed / mean(lowess_smoothed)
  })
  na.smoothed = unlist(na.smoothed)
  
  # Update NAs
  colony.size.data[ind.na] = colony.size.data[ind.na] / as.vector(na.smoothed)
  
  return(colony.size.data)
}

# Jackknife filter (LOOCV): checks for colonies that contribute more than 90% of total variance in their replicates
# @param plate.data: SGA formatted data frame
# @param field.to.filter: name of the column in the data to filter
# @return jk.ignore.logical: logical array with TRUE for rows to ignore. Status code as name
jackknifeFilter <- function(plate.data, field.to.filter){
  
  loginfo('# Applying jackknife filter')
  
  # Status code of filter
  status.code = 'JK'
  
  # Get all unique queries
  uniq.query = unique(plate.data$query)
  
  # Get all unique arrays
  uniq.array = unique(plate.data$array)
  uniq.spots = unique(plate.data$spots)
  
  # Remove HIS3 from arrays
  uniq.array = uniq.array[ ! grepl('YOR202W', uniq.array, ignore.case=T) ]
  uniq.spots = uniq.spots[ ! grepl('YOR202W', uniq.array, ignore.case=T) ]
  
  # Function used for jackknife function (sd, ignoring NA)
  theta <- function(x){sd(x, na.rm=T)}
  
  jk.ignore = lapply(uniq.spots, function(curr.spot){
    
    # Ignore HIS3 from arrays
    # if( grepl('YOR202W', curr.array, ignore.case=T) ) NULL
    
    curr.ind = which(plate.data$spot == curr.spot)
    # Get indices of our current array
    vals = plate.data[[field.to.filter]][curr.ind]
    # Get NA values
    ind.na = is.na(vals)
    
    # If we dont have enough non-NA values
    if( sum(!is.na(vals))  < 2 ) NULL
    
    # Get jackknife variances 
    jk.vals = jackknife(vals, theta)$jack.values
    
    # Get total variance and jackknife variances
    total.var = var(vals, na.rm=T) * (length(vals)-1)
    jk.var =  (jk.vals^2) * (length(jk.vals)-2)
    
    # Find colonies that contribute more than 90% of total variance
    t = which( (total.var - jk.var) >  (0.9*total.var) )
    curr.ind[t]
  })
  
  # These are the indicies to be ignored
  jk.ignore.ind = unlist(jk.ignore[! sapply(jk.ignore, is.null)])
  
  # Turn into logical values i.e. if we had indicies 1, 3 to be ignored, 
  # it will be converted to TRUE FALSE TRUE FALSE FALSE ......
  jk.ignore.logical = 1:nrow(plate.data) %in% jk.ignore.ind
  
  # Add status code JK - jackknife failed
  names(jk.ignore.logical) = NA
  names(jk.ignore.logical)[jk.ignore.ind] = rep(status.code, length(jk.ignore.ind))
  
  loginfo('Done jackknife filter, total ignored = %d', sum(jk.ignore.logical))
  return(jk.ignore.logical)
}

# Big replicates filter: remove excessively large colonies replicates
# @param plate.data: SGA formatted data frame
# @param field.to.filter: name of the column in the data to filter
# @param max.colony.size: "large" colony threshhold, usually 1.5 * median of plate
# @return big.logical: logical array with TRUE for rows to ignore. Status code as name
bigReplicatesFilter <- function(plate.data, field.to.filter, max.colony.size, ignore.ind){
  
  loginfo('# Applying big replicates filter, max.colony.size = %d', max.colony.size)
  # Status code of filter
  status.code = 'BG'
  
  #Find indicies of large colonies
  large.ind = which(plate.data[[field.to.filter]] >= max.colony.size)
  
  # Gets spots of large colonies, and returns the count of each spot
  big.spots = table(plate.data$spots[large.ind])
  
  # Get colonies such that their spot contains at least 3 big colonies
  big.spots = big.spots[big.spots >= 3]
  spots.to.remove = names(big.spots)
 
  # Get which colonies are in spots to remove
  big.ind = which(plate.data$spots %in% spots.to.remove)
  
  big.logical = 1:nrow(plate.data) %in% big.ind
  
  # Set status code BG BIG REP?
  names(big.logical) = NA
  names(big.logical)[big.ind] =  rep(status.code, length(big.ind))
  
  loginfo('Done big replicates filter, total ignored = %d', sum(big.logical))
  return(big.logical)
}


####################################################################################
# Filter functions used in spatial normalization: rewritten from matlab for R 

# Returns a gaussian filter matrix with dimensions x by x: equal to fspecial function in matlab
# Inputs:
# x = dimensions (number of rows/cols) of the returned gaussian filter
#	sigma = standard deviation
# The fgaussian function was edited from the original
fgaussian <- function(x, sigma){
  x = SGAseq(x)
  mat = matrix(NA, length(x),length(x));
  for(i in 1:length(x)){
    for(j in 1:length(x)){
      n1 = x[i];
      n2 = x[j];
      mat[i,j] = exp(-(n1^2+n2^2)/(2*sigma^2));
    }
  }
  mat = mat/sum(mat)
  return(mat)
}


# Average filter
faverage <- function(size){
  x = 1/(size*size)
  ret = matrix(rep(x, size*size), size,size)
  return(ret)
}

# Helper function for fgaussian - given some value x, returns a gradient array begining with 0 on the inside and increasing outwards. Example: x = 7 returns [3,2,1,0,1,2,3] 
# Inputs:
#	x = number of elements in the returned array
# The seq function was edited from the original
SGAseq <- function(x){
  n = x;
  x = c(1:x)
  if(n%%2){
    rhs = x[1:floor(length(x)/2)];
    lhs = rev(rhs);
    return(c(lhs,0,rhs))
  }else{
    rhs = x[1:floor(length(x)/2)] - 0.5;
    lhs = rev(rhs);
    return(c(lhs,rhs))
  }
}

# Applies a filter to a matrix: see imfilter (matlab) with replicate option
# Inputs:
#	mat = matrix to which the filter is applied
# filter = a square matrix filter to be applied to the matrix 
applyfilter <- function(mat, filter, padding_type = 'zeros'){
  mat2 = mat
  fs = dim(filter);
  if(fs[1] != fs[2])
    stop('Filter must be a square matrix')
  if(fs[1] %% 2 == 0)
    stop('Filter dimensions must be odd')
  if(fs[1] == 1)
    stop('Filter dimensions must be greater than one')
  
  x = fs[1];
  a = (x-1)/2;
  
  s = dim(mat2)
  r = matrix(0, s[1], s[2])
  
  start = 1+a;
  end_1 = s[1]+a;
  end_2 = s[2]+a;
  
  mat2 = padmatrix(mat, a, padding_type)
  
  for(i in start:end_1){
    for(j in start:end_2){
      temp = mat2[(i-a):(i+a), (j-a):(j+a)] * filter;
      r[(i-a),(j-a)] = sum(temp)
    }
  }
  return(r)
}

# Applies a filter to a matrix: see imfilter (matlab) with replicate option
# Inputs:
#	mat = matrix to which the filter is applied
# dim = number of rows/cols of window
medianfilter2d <- function(mat, dim, padding_type = 'zeros'){
  mat2 = mat
  fs = c()
  fs[1] = dim
  fs[2] = dim
  
  if(fs[1] != fs[2])
    stop('Filter must be a square matrix')
  if(fs[1] %% 2 == 0)
    stop('Filter dimensions must be odd')
  if(fs[1] == 1)
    stop('Filter dimensions must be greater than one')
  
  x = fs[1];
  a = (x-1)/2;
  
  s = dim(mat2)
  r = matrix(0, s[1], s[2])
  
  start = 1+a;
  end_1 = s[1]+a;
  end_2 = s[2]+a;
  
  mat2 = padmatrix(mat, a, padding_type)
  
  for(i in start:end_1){
    for(j in start:end_2){
      temp = mat2[(i-a):(i+a), (j-a):(j+a)];
      r[(i-a),(j-a)] = median(temp)
    }
  }
  return(r)
}

# Adds a padding to some matrix mat such that the padding is equal to the value of the nearest cell
# Inputs:
#	mat = matrix to which the padding is added
#	lvl = number of levels (rows/columns) of padding to be added
#	padding = type of padding on the matrix, zero will put zeros as borders, replicate will put the value of the nearest cell
padmatrix <- function(mat, lvl, padding){
  s = dim(mat);
  row_up = mat[1,]
  row_down = mat[s[1],]
  
  if(padding == 'zeros'){
    row_up = rep(0, length(row_up))
    row_down = rep(0, length(row_down))
  }
  #Add upper replicates
  ret = t(matrix(rep(row_up, lvl), length(as.vector(row_up))))
  #Add matrix itself
  ret = rbind(ret, mat)
  #Add lower replicates
  ret = rbind(ret, t(matrix(rep(row_down, lvl), length(as.vector(row_down)))))
  
  #Add columns
  s = dim(ret);
  col_left = ret[,1]
  col_right = ret[,s[2]]
  
  if(padding == 'zeros'){
    col_left = rep(0, length(col_left))
    col_right = rep(0, length(col_right))
  }
  
  #Add left columns
  ret2 = matrix(rep(col_left, lvl), length(as.vector(col_left)))
  #Add matrix itself
  ret2 = cbind(ret2, ret)
  #Add right columns
  ret2 = cbind(ret2, matrix(rep(col_right, lvl), length(as.vector(col_right))))
  
  #return 
  return(ret2)
}

```


# Experiment Description

The screen is conducted in 1536-colony format with 4 replicates per query in a 2x2 group.

```{r Experiment description, echo = FALSE}
ExpCondition <- data.frame(rep(c("Query 1"), each = 2),
                            rep(c("Query 1"), each = 2),
                            rep(c("Query 2"), times = 2),
                            rep(c("Query 2"), times = 2))
names(ExpCondition) <- c("", "", "", "")

knitr::kable(ExpCondition, col.names = NULL,
             caption = paste("Query layout")) %>%
  kable_styling(c("bordered"), full_width = FALSE,
                position = "left", font_size = 16)

```


# Data Processing

The analysis begins with loading the colony size data obtained from image segmentation done in the pre-analysis markdown file.

```{r Data load 1}
# Load the colony size data from photograph segmentation
load("dataTableRaw.rda")
head(df3)

# Modify the table into a long form for simpler analysis.
df_mod <- df3 %>%
  select(-contains(c("circularityImg", "mNG_lo","mNG_hi"))) %>%
  mutate(size = sizeImg,
         row = alphabet1536_rev[row_1536],
         col = col_1536) %>%
  select(row, col, 1:27,31)


  
```


## Remove Border Colonies and Mark Missing Positions


The colony sizes which are 0 are changed to NA as they are already known to be empty positions. This change is based on the prePin data and will be applied to the whole dataset so, if there is growth occurring in that position during the experiment, it is not quantified as a real colony. Then, border colonies are marked, and the colony sizes are also set to NA.

```{r Border n empty spot filtering 2}
# Modify the border colonies table
BorderColonies2 <- BorderColonies %>%
  mutate(IsBorder = TRUE)

# Combining the data table with the border colonies list to mark border and
# non-border colonies
df_woBorder <- left_join(df_mod, BorderColonies2,
                                     by = "well")
df_woBorder$IsBorder[is.na(df_woBorder$IsBorder)] <- FALSE

# Marking empty positions from the crossing
df_woBorder_2 <- df_woBorder %>%
  mutate(IsEmpty_Hyg = size == 0) %>%
  # Set the empty positions size as NA
  mutate(size = ifelse(IsEmpty_Hyg, NA_integer_, size)) %>%
  # Set the border colonies size as NA
  mutate(size = ifelse(IsBorder, NA_integer_, size))

```


## Data Correction and Normalization

Here we use SGA Tools (Omar Wagih et al., 2013) to correct the data for plate and spatial effects as follows:

1) plateNormalization 1 (pnorm): Normalises colony sizes to the plate median per plate.

2) spatialNormalization (snorm): Normalizes pnorm colony sizes on each plate using a gaussian filter to normalize for any gradient effect on the plate via median smoothing.

4) plateNormalization 2 (pnorm2): Re-normalises rcnorm colony sizes to the plate median of all plates.


Row and Column normalization was not performed due to overcompensation of "normal size" colonies in rows and column with a lot of hits, that is, small colonies.

### Data Normalization

The SGA Tools normalizeSGA function is executed. No median normalization is done beforehand since the strains are not crossed with different query types.
SGA normalization is not done for the hygromycin recovery step since that is only to determine empty positions (i.e. failed crosses).

```{r Normalization setup}
# Plates description
nPlates <- 96
nROW <- 32
nCOL <- 48

# Overall experiment plate median is set to be able to compare between different
# conditions/plates.
KOPM <- median(df_woBorder_2$size, na.rm = T)

```

The normalization is done for `r paste(nPlates)` plates, with `r paste(nROW)` rows and `r paste(nCOL)` columns in a `r paste(nROW*nCOL)` format.

```{r Data normalization, results = 'hide'}
# Normalization of the data based on median adjusted size
DataInput <- df_woBorder_2


# Temporary table to hold the data for normalization with just the column names
retA <- data.frame(DataInput[0, ])

# Normalize the remaining plates.

    for(i in 1:nPlates){
      print(paste0("plate=",i))
      
      retB <- DataInput %>% filter(plate_1536 == i)
      
      retC <- normalizeSGA(plate.data = retB, overall.plate.median = KOPM,
                           field.to.normalize = "size")
      
      retA <- rbind(retA, retC)
    }


#To calculate the library median using only the values between 0.4 and 0.8 of the size distribution
vals = sort(retA$pnorm2)
length = length(vals)
lower = 0.4
upper = 0.8


# Return the normalized data from the temporary table to a permanent table.
df_normalized <- retA %>%
  #mutate(pnorm2 = ifelse(pnorm2 <= 0, NA_integer_, pnorm2)) %>%
  group_by(plate,sample_type, Fbox) %>%
  # Adjust the values according to the median of the plate
  mutate(#platemedian1 = median(pnorm2, na.rm = T),
         platemedian2 = median(vals[round(lower*length):round(upper*length)], na.rm = T),
         pnorm3 = pnorm2/platemedian2) %>%
  ungroup() %>%
  do({
    tempA <- .
    # Change the NAs to zero for the jackknife function.
    tempA$pnorm3[is.na(tempA$pnorm3)] <- 0
    tempB <- tempA %>%
      # Performing jackknife marking here. It checks for colonies that contribute
      # more than 90% of total variance in their replicates and marks as TRUE
      group_by(plate,sample_type, Fbox, ORF) %>%
      mutate(JK = jackknife(pnorm3, theta)$jack.values,
             total.var = var(pnorm3, na.rm=T) * (length(pnorm3)-1),
             jk.var =  (JK^2) * (length(JK)-2),
             JK.TRUE = (total.var - jk.var) > (0.9*total.var)) %>%
      ungroup()
    tempB
  }) %>%
  ungroup()

save(df_normalized, file = "df_normalized")


```


### Illustrating Normalization Outcome

Colony sizes are shown here by plate with boxplots to illustrate how the normalization affects the data for each step.

```{r Post-normalization boxplot, fig.height = 24}


ggplot(data = df_normalized) + 
  geom_violin(aes(as.factor(plate), size)) + 
  facet_wrap(vars(sample_type, Fbox)) +
  labs(x = "Plate", y = "Colony Size", 
       title = "Raw colony size") +
  theme(axis.text.x = element_text(angle = -45))

ggplot(data = df_normalized) + 
  geom_boxplot(aes(as.factor(plate), pnorm)) +
  facet_wrap(vars(sample_type, Fbox)) +
  labs(x = "Plate", y = "Colony Size", 
       title = "Colony size (SGA Tools - pnorm)") +
  theme(axis.text.x = element_text(angle = -45))

ggplot(data = df_normalized) + 
  geom_boxplot(aes(as.factor(plate), snorm)) +
  facet_wrap(vars(sample_type, Fbox)) +
  labs(x = "Plate", y = "Colony Size", 
       title = "Colony size (SGA Tools - snorm)") +
  theme(axis.text.x = element_text(angle = -45))


ggplot(data = df_normalized) + 
  geom_boxplot(aes(as.factor(plate), pnorm2)) +
  facet_wrap(vars(sample_type, Fbox)) +
  labs(x = "Plate", y = "Colony Size",
       title = "Colony size (SGA Tools - pnorm2)") +
  theme(axis.text.x = element_text(angle = -45))



```

With the data normalized, the whole data frame is simplified for further analysis.

```{r Normalization end}
# Simplify the normalized data table
# Remove the empty border rows and remove locations where the ORF was not present in the final recovery step.
df_norm_simplified <- df_normalized %>%
  select(plate, well, ORF, gene, sample_type, Fbox, size, IsBorder, IsEmpty_Hyg, JK.TRUE, pnorm3) %>%
  filter(!IsBorder, !IsEmpty_Hyg, sample_type != "fluor_control",sample_type != "control")

ggplot(data = df_norm_simplified) + 
  geom_boxplot(aes(as.factor(plate), pnorm3)) +
  facet_wrap(vars(sample_type, Fbox)) +
  labs(x = "Plate", y = "Colony Size", 
       title = "Colony size (Normalized by plate median)") +
  theme(axis.text.x = element_text(angle = -45))


```


# Data Analysis


In this section, the difference between each ORF and the median per plate adjusting for the deviation per plate is calculated as the Z-score. The colonies that contribute to more than 90% of total variance in their replicates are not used for any calculations.

A t-test is then calculated to determine the significance of this difference. The p-values are corrected for multiple testing using the Benjamini-Hochberg method. The results are shown as volcano plots with the significant hits listed in the tables below each plot.

```{r Data analysis 1 - Z-score}
#To restrict the calculations between the values between 0.4 and 0.8 of the size distribution.
vals = sort(df_norm_simplified$pnorm3)
length = length(vals)
lower = 0.4
upper = 0.8

# Dividing the colony sizes by the median of all colonies for that query
df_norm_platevalues <- df_norm_simplified %>%
  group_by(plate, Fbox) %>%
  summarise(plate_median1 = median(vals[round(lower*length):round(upper*length)], na.rm = T),
            plate_mad1 = mad(vals[round(lower*length):round(upper*length)], na.rm = T)) %>%
  ungroup()

# Changing NA positions to zero for T-test calculations.
df_norm_2 <- df_norm_simplified %>%
  mutate(pnorm3 = ifelse(is.na(pnorm3), 0, pnorm3))

# Calculating the z-score
df_norm_3 <- df_norm_2 %>%
  filter(!JK.TRUE) %>%
  left_join(., df_norm_platevalues, by = c("plate", "Fbox")) %>%
  mutate(delta_size_score = (pnorm3 - plate_median1) / plate_mad1)


```

After calculating the z-score, the distribution of each plate/stage is shown below. It was also calculated a distribution for the Z-score calculated for the first and second replicate together (second graphic). From now on, everytime there is only one graphic despicted for "IAA" and "control" is with both replicate one and two together.

```{r Data analysis 2 - Z-score}
ggplot() +
  geom_histogram(data = df_norm_3, aes(delta_size_score), bins = 200) +
  facet_wrap(vars(Fbox)) +
  labs(x = "Z-score",
       subtitle = "Z-score = (normalised size - plate median) / plate MAD")

```

From the graphs above, we can already see that treatment with IAA leads to a decrease in fitness for some ORFs and has a stronger effect than the control plate.




```{r Data analysis 3 - Z-score}

save(df_norm_3, file = "df_norm_3")
load("df_norm_3")

```


# Data Visualisation

## Volcano plot

Several thresholds are set here.

```{r Thresholds 1}
# Change these value to the desired threshold. Currently the settings are:

# Z-score threshold
sizeThresLow <- -10

# Quotient Threshold
sizeThresquot <- 0.80

# BH corrected p-value
BHThres <- 0.05

```



```{r Outcome of Hits _ Quotient and p_value}

# Library strains
Strains_IAA_d1_quot <- df_norm_3 %>%
  filter(Fbox == FALSE) %>%
  #Filter
  group_by(plate, ORF, gene) %>%
  filter(n() > 1) %>%
  #Calculate Medians and p-value
  summarise(size_median = median(pnorm3, na.rm = T)) %>%
  drop_na(ORF) %>%
  select(plate, ORF, gene,size_median)
Strains_IAA_d1_quot <- Strains_IAA_d1_quot[!duplicated(Strains_IAA_d1_quot$ORF),]

#Calculate p-value


#All Genes p-value

Library_v1_final <- df_norm_3 %>%
    select(-sample_type) %>%
  # Separate in two tables
  do ({
    tmp <- subset(., Fbox == T) %>% select(-Fbox)
    tmp2 <- subset(., Fbox == F) %>% select(-Fbox)
    full_join(tmp, tmp2, by = c("plate", "ORF", "gene", "well"),
              suffix = c("_IAA", "_ctrl"))
  }) %>%
  #Filter
  group_by(plate, ORF, gene) %>%
  filter(n() > 1) %>%
  #Calculate Medians and p-value
  summarise(size_median_IAA = median(pnorm3_IAA, na.rm = T),
            size_median_ctrl = median(pnorm3_ctrl, na.rm = T),
            pvaluesize = tryCatch({t.test(x = pnorm3_IAA, y = pnorm3_ctrl, paired = FALSE)$p.value}, error = function(e){NA}),
            df_size = tryCatch({t.test(x = pnorm3_IAA, y = pnorm3_ctrl, paired = FALSE)$parameter}, error = function(e){NA})) %>%
  #Adjust p-value
  mutate(BHsize = p.adjust(pvaluesize, method = "BH")) %>%
  #Calculate quotients
  mutate(delta_size_quot = size_median_IAA/size_median_ctrl) %>%
  drop_na(ORF) %>%
  select(plate, ORF, gene,size_median_IAA,size_median_ctrl, delta_size_quot, BHsize)
Library_v1_final <- Library_v1_final[!duplicated(Library_v1_final$ORF),]
Library_v1_final <- Library_v1_final %>% drop_na(BHsize)

# Hits considering of the p-value
Hits_IAA_d1_quot <- df_norm_3 %>%
    select(-sample_type) %>%
  # Separate in two tables
  do ({
    tmp <- subset(., Fbox == T) %>% select(-Fbox)
    tmp2 <- subset(., Fbox == F) %>% select(-Fbox)
    full_join(tmp, tmp2, by = c("plate", "ORF", "gene", "well"),
              suffix = c("_IAA", "_ctrl"))
  }) %>%
  #Filter
  group_by(plate, ORF, gene) %>%
  filter(n() > 1) %>%
  #Calculate Medians and p-value
  summarise(size_median_IAA = median(pnorm3_IAA, na.rm = T),
            size_median_ctrl = median(pnorm3_ctrl, na.rm = T),
            pvaluesize = tryCatch({t.test(x = pnorm3_IAA, y = pnorm3_ctrl, paired = FALSE)$p.value}, error = function(e){NA}),
            df_size = tryCatch({t.test(x = pnorm3_IAA, y = pnorm3_ctrl, paired = FALSE)$parameter}, error = function(e){NA})) %>%
  #Adjust p-value
  mutate(BHsize = p.adjust(pvaluesize, method = "BH")) %>%
  #Calculate quotients
  mutate(delta_size_quot = size_median_IAA/size_median_ctrl) %>%
  filter(delta_size_quot <= sizeThresquot,
         BHsize < BHThres) %>%
  drop_na(ORF) %>%
  select(plate, ORF, gene,size_median_IAA,size_median_ctrl, delta_size_quot, BHsize)
Hits_IAA_d1_quot <- Hits_IAA_d1_quot[!duplicated(Hits_IAA_d1_quot$ORF),]

save(Hits_IAA_d1_quot, file = "Library_v1_final")
save(Hits_IAA_d1_quot, file = "mNG_Size_Hits_SGA")

```


# Fluorescence Measurements analysis

### Threshold

It is important first to define thresholds. For the corrected p-value, a value of 0.05 or 0.01 should be used.

```{r Threshold}

#p-value
BHcorrect5 <- 0.05
BHcorrect1 <- 0.01

#Above background
Detec_protein <- 1.2

#Size difference
Size_diference <- 0.8

#Considered degradation
Deg <- 0.5


```

### Comparison with previous data

Since the library was made using the C-SWAT library, there are already published value for the intensity of each protein. For that reason, the right comparison that it will be done is the fluorescent levels between the published data (https://www.nature.com/articles/s41592-018-0045-8). The data used is from the mNG-II.

```{r Comparison with C-SWAT library}

#Uploading the database
Protein_Abund_CSWAT <- read.xlsx(xlsxFile = "06_datasets/20220914_Protein_Abundance_CSWAT.xlsx", sheet = "Table S2")
Protein_Abund_CSWAT_inLib <- semi_join(Protein_Abund_CSWAT, df_final, by = "ORF")

#Test if we obtain the same fluorescence measurements as the published ones

Abundance <- left_join(df_final, Protein_Abund_CSWAT_inLib, by = "ORF") %>% mutate(log2intensity = log2(intensity)) 

Abundance1 <- Abundance %>% drop_na(log2intensity) %>% 
                          drop_na(log2mNG_final_ctrl) %>%
                          filter(is.finite(log2intensity)) %>% 
                          filter(is.finite(log2mNG_final_ctrl)) %>%
                        filter(log2mNG_final_ctrl >= -15) %>%
                        filter(log2intensity >= -30)

#Figure S2c
ImS2c <- ggplot(data = Abundance1) +
  geom_point(aes(log2mNG_final_ctrl, log2intensity)) +
      theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      labs(x = "log2(ORF-mNG-AID*-3Myc)",
           y = "log2(C-SWAT mNG-II)")
print(ImS2c)

ggsave("05_plots/02-Figure_S2c.pdf",
    height = 3.8,
    width = 6)

Pcoef <- cor.test(Abundance1$log2mNG_final_ctrl, Abundance1$log2intensity, method ="pearson")

Pcoef

Scoef <- cor.test(Abundance1$log2mNG_final_ctrl, Abundance1$log2intensity, method ="spearman", exact = FALSE)

Scoef

```



```{r}
#Uploading Database Essential Genes
Ess_Genes <- read.xlsx(xlsxFile = "06_datasets/20200224_Essential_ORFs.xlsx", sheet = "20200224_Essential_ORFs") %>%
  `colnames<-`(c("ORF", colnames(.)[2:6]))
Ess_Genes_inLib <- semi_join(Ess_Genes, df_final, by = "ORF")

```


```{r}

#Essential Genes
Scatter <- rbind((anti_join(df_final, Ess_Genes_inLib , by = "ORF") %>% mutate(Gene = "Not essential")), (semi_join(df_final, Ess_Genes_inLib , by = "ORF") %>% mutate(Gene = "Essential")))

#Figure Figure S3a
ImS3a <- ggplot(Scatter) +
      geom_point(aes(log2mNG_norm_ctrl, log2mNG_norm_deg, color=Gene), alpha = 0.3, size = 2) +
       theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
      labs(x = "log2(Normalized fluorescent value - no protein degradation)",
           y = "log2(Normalized fluorescent value - protein degradation)")
print(ImS3a)
ggsave("05_plots/03-Figure_S3a.pdf",
    height = 8,
    width = 16)


ggplot(Scatter) +
  geom_density(aes(x = log2mNG_norm_ctrl, group=Gene, fill = Gene))

ggplot(Scatter) +
  geom_density(aes(x = log2mNG_norm_deg, group=Gene, fill = Gene))
```

```{r}

#Fitness Defect genes
Size_Hits <- Hits_IAA_d1_quot

Size <-rbind((anti_join(df_final, Size_Hits , by = "ORF") %>% mutate(Size = "Not affected")), (semi_join(df_final, Size_Hits , by = "ORF") %>% mutate(Size = "Fitness defect")))

#Figure S3b
ImS3b <- grid.arrange(
    ggplot(Size) +
      geom_point(aes(log2mNG_norm_ctrl, log2mNG_norm_deg, color=Size), alpha = 0.3) +
     labs(title = "ORFs maked in blue have less then 80% in size compaired with the control"),
    top = textGrob(paste("Fluorescence intensity of in ORF-mNG-AID degraded vs non degraded"),
                   gp = gpar(fontsize = 24)))
print(ImS3b)

ggsave("05_plots/04-Figure_S3b.pdf",
       height = 8,
    width = 16)


ggplot(Size) +
  geom_density(aes(x = log2mNG_norm_ctrl, group=Size, fill = Size))

ggplot(Size) +
  geom_density(aes(x = log2mNG_norm_deg, group=Size, fill = Size))


```


```{r}

#Categorization in the 4 Categories

Categorizationp5 <- rbind(significant %>% filter((mNG_ORFnorm_ctrl) <= Detec_protein) %>% mutate(Category = "Not detectable"),
                        significant %>% filter((mNG_ORFnorm_deg) <= Detec_protein & mNG_ORFnorm_ctrl > Detec_protein & (mNG_final_val_deg/mNG_final_val_ctrl) < Deg & BHmNG < BHcorrect5) %>% mutate(Category = "Complete degradation"),
                        significant %>% filter((mNG_final_val_deg/mNG_final_val_ctrl) < Deg & (mNG_ORFnorm_deg) > Detec_protein & mNG_ORFnorm_ctrl > Detec_protein & BHmNG < BHcorrect5) %>% mutate(Category = "Partial degradation"), 
                        significant %>% filter((mNG_final_val_deg/mNG_final_val_ctrl) >= Deg & mNG_ORFnorm_ctrl > Detec_protein | mNG_ORFnorm_ctrl > Detec_protein & BHmNG >= BHcorrect5) %>% mutate(Category = "No degradation")) %>%
  select(plate, ORF, gene, mNG_ORFnorm_ctrl, mNG_final_ctrl, log2mNG_norm_ctrl, log2mNG_final_ctrl, mNG_ORFnorm_deg, mNG_final_deg, log2mNG_norm_deg, log2mNG_final_deg, BHmNG, Category)
Categorizationp5 <- Categorizationp5[!duplicated(Categorizationp5$ORF),]


#Figure 2d
Im2d <- ggplot(Categorizationp5) +
      geom_point(aes(log2mNG_norm_ctrl, log2mNG_norm_deg, color=Category), alpha = 0.3, size = 2) +
      theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
      labs(x = "log2(Normalized fluorescent value - no protein degradation)",
           y = "log2(Normalized fluorescent value - protein degradation)")
print(Im2d)

ggsave("05_plots/05-Figure_2d.pdf",
       height = 8,
    width = 16)

ggplot(Categorizationp5) +
  geom_density(aes(x = log2mNG_norm_ctrl, group = Category, fill = Category))

ggplot(Categorizationp5) +
  geom_density(aes(x = log2mNG_norm_deg, group = Category, fill = Category))

Non_Detectable5 <- Categorizationp5 %>% filter(Category == "Not detectable") %>%
  select(plate, ORF, gene, mNG_ORFnorm_ctrl, mNG_final_ctrl, log2mNG_norm_ctrl, log2mNG_final_ctrl, mNG_ORFnorm_deg, mNG_final_deg, log2mNG_norm_deg, log2mNG_final_deg, Category)

Complete_degradation5 <- Categorizationp5 %>% filter(Category == "Complete degradation")   %>%
  select(plate, ORF, gene, mNG_ORFnorm_ctrl, mNG_final_ctrl, log2mNG_norm_ctrl, log2mNG_final_ctrl, mNG_ORFnorm_deg, mNG_final_deg, log2mNG_norm_deg, log2mNG_final_deg, Category)

Partial_degradation5 <- Categorizationp5 %>% filter(Category == "Partial degradation")  %>%
  select(plate, ORF, gene, mNG_ORFnorm_ctrl, mNG_final_ctrl, log2mNG_norm_ctrl, log2mNG_final_ctrl, mNG_ORFnorm_deg, mNG_final_deg, log2mNG_norm_deg, log2mNG_final_deg, Category)

No_degradation5 <- Categorizationp5 %>% filter(Category == "No degradation")  %>%
  select(plate, ORF, gene, mNG_ORFnorm_ctrl, mNG_final_ctrl, log2mNG_norm_ctrl, log2mNG_final_ctrl, mNG_ORFnorm_deg, mNG_final_deg, log2mNG_norm_deg, log2mNG_final_deg, Category)

```


## Further Analysis

### Protein Localization

We want to see now what is the protein localization of each of the categories, that is, Complete, Partial and no degradation. For this, a database published in the paper https://www.nature.com/articles/nature02026. The dataset was downloaded at 12/09/2022.

```{r Protein Localization Database}

Prot_Loc <- read.xlsx(xlsxFile = "06_datasets/20220912_GFPLocalizationLibrary.xlsx", sheet = "allOrfData", sep.names = " ") %>%
  `colnames<-`(c("orfid", colnames(.)[2:32]))
Prot_Loc_inLib <- semi_join(Prot_Loc, df_final, by = "ORF")

```

Now we want to attribute to each of the genes their localization. First for the values with a threshold of p-value < 0.05
```{r Protein localization Plots - p-value - 0.05 without Non-Detectable}

#Attribute each ORF to a cell localization
ORF_Loc5 <- left_join(Categorizationp5[c("ORF")], Prot_Loc_inLib, by = "ORF", suffix = c(".x",".y"))
ORF_Loc5 <- as.data.frame(ORF_Loc5)

#Remove duplicated rows
ORF_Loc5 <- ORF_Loc5[!duplicated(ORF_Loc5$ORF),]
rownames(ORF_Loc5) <- ORF_Loc5$ORF

#Quantify how many of each cell localization there is per Category
cell_loc5 <- ORF_Loc5[10:length(colnames(ORF_Loc5))]
cell_loc5 <- cell_loc5 %>% mutate(across(everything(), as.logical))
cell_loc5$ORF <- rownames(cell_loc5)
cell_loc5 <- left_join(cell_loc5, Categorizationp5[c("ORF", 'Category')], by="ORF")
cell_loc5 <- cell_loc5 %>% select(-c(ORF)) %>% group_by(Category) %>% summarize_all(sum, na.rm=TRUE) 
colSums(cell_loc5 %>% select(-c( Category)), na.rm = TRUE)
cell_loc5_D <- cell_loc5 %>% filter(Category != "Not detectable")

#Calculate the percentage of each cell localization per Category
cell_loc5_perc_D <- adorn_percentages(cell_loc5_D, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(cell_loc5_D, caption = "Localization of every ORF by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(cell_loc5_perc_D, caption = "Localization of every ORF by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
cell_loc5_plot_D <- cell_loc5_perc_D %>% pivot_longer(!Category, names_to = "cellular_comp", values_to = "amount")
cell_loc5_plot_D$Category <- factor(cell_loc5_plot_D$Category,levels = c("Complete degradation","Partial degradation","No degradation"))

#Plot
ggplot(data = cell_loc5_plot_D) +
  geom_col(aes(x = cellular_comp, y = amount, fill = Category), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category",
       title = "Percentage of ORFs in different cellular compartments",
       subtitle = "p-value < 0.05")

ggplot(data = cell_loc5_plot_D) +
  geom_bar(aes(x = cellular_comp, y = amount, fill = Category), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category",
       title = "Percentage of ORFs in different cellular compartments",
       subtitle = "p-value < 0.05")


#Figure 2e
Im2e <- ggplot(data = cell_loc5_plot_D) +
  geom_bar(aes(x = cellular_comp, y = amount,  fill = Category), position = "fill", stat = "identity") +
      theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category")

print(Im2e)

ggsave("05_plots/06-Figure_2e.pdf",
       height = 16,
    width = 12)


```

The plots show that there is a tendency of having more complete degraded protein in cellular localization that do not have any membranes. The mitochondria is the organelle where there is more proteins without any degradation.

### C-terminal domain availability in membrane protein

We want to investigate if the hits are related with the fact that the C-terminal domain is available or not to be recognize by the degradation system. For that, the dataset of hit was compared with a dataset of membrane protein were the C-terminal domain are known to be cytosolic of in the lumen. The dataset was downloaded at 2022-09-13 in the website https://www.pnas.org/doi/10.1073/pnas.0604075103.


```{r C-terminal domain Database}

C_terminal_domain <- read.xlsx(xlsxFile = "06_datasets/20220913_CterminalLocalization.xlsx", sheet = "Table 2 Data")
C_terminal_domain_inLib <- semi_join(C_terminal_domain, df_final, by = "ORF") %>% 
  select(ORF,Gene.name,Cterm)

```

```{r C-terminal domain Plots - p-value - 0.05 without Non-Detectable }

#Attribute each ORF to a cell localization
ORF_Cterm5 <- inner_join(Categorizationp5[c("ORF")], C_terminal_domain_inLib, by = "ORF", suffix = c(".x",".y"))
ORF_Cterm5 <- as.data.frame(ORF_Cterm5)

#Remove duplicated rows
ORF_Cterm5 <- ORF_Cterm5[!duplicated(ORF_Cterm5$ORF),]
rownames(ORF_Cterm5) <- ORF_Cterm5$ORF

#Filter ORFs with no information

ORF_Cterm5 <- ORF_Cterm5 %>% filter(Cterm != "-")

#Quantify how many of each cell localization there is per Category
ORF_Cterm5$Cterm[ORF_Cterm5$Cterm == "in*"] <- "in"

ORF_Cterm5$Cytosol <- ifelse(ORF_Cterm5$Cterm == "in", TRUE, FALSE)
ORF_Cterm5$Lumen <- ifelse(ORF_Cterm5$Cterm != "in", TRUE, FALSE)




cterm5 <- left_join(ORF_Cterm5, Categorizationp5[c("ORF", 'Category')], by="ORF")
cterm5 <- cterm5 %>% select(-c(ORF,Gene.name,Cterm)) %>% group_by(Category) %>% summarize_all(sum, na.rm=TRUE) 
colSums(cterm5 %>% select(-c( Category)), na.rm = TRUE)

cterm5_D <- cterm5 %>% filter(Category != "Not detectable")

#Calculate the percentage of each cell localization per Category
cterm5_perc_D <- adorn_percentages(cterm5_D, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(cterm5_D, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(cterm5_perc_D, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
cterm5_plot_D <- cterm5_perc_D %>% pivot_longer(!Category, names_to = "Membrane_pos", values_to = "amount")
cterm5_plot_D$Category <- factor(cterm5_plot_D$Category,levels = c("Complete degradation","Partial degradation","No degradation"))

#Plot
ggplot(data = cterm5_plot_D) +
  geom_col(aes(x = Membrane_pos, y = amount, fill = Category), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category",
       title = "C-terminal Position of the ORF in the Cellular Membrane",
       subtitle = "p-value < 0.05")



ggplot(data = cterm5_plot_D) +
  geom_bar(aes(x = Membrane_pos, y = amount, fill = Category), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category",
       title = "C-terminal Position of the ORF in the Cellular Membrane",
       subtitle = "p-value < 0.05")


#Figure 2e
Im2e2 <- ggplot(data = cterm5_plot_D) +
  geom_bar(aes(x = Membrane_pos, y = amount, fill = Category), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category")

print(Im2e2)

ggsave("05_plots/07-Figure_2e.pdf",
       height = 8,
    width = 12)

```

We can conclude that the protein that have the C-terminal inside the lumen have more difficulty being degraded by the proteosome, as expected.

### Protein Abundance

We now want to understand if the abundance of a protein has any influence in the outcome, that is, if a high expressed protein is less likely to be completely degraded compared to a low expressed protein. As it was before, the value obtained for the non degraded protein will be used as protein abundance. For this analysis, the Non Detectable protein are excluded.

#### Bins according to Protein abundance

Now we divide the bins for having the some protein abundance.

```{r Abundance bins p-value 0.05}

Abund5 <- Categorizationp5 %>% filter(Category != "Not detectable")

#Remove duplicated rows
Abund5 <- Abund5[!duplicated(Abund5$ORF),]

#Order by mNG_final_ctrl
Abund5 <- Abund5[order(Abund5$log2mNG_final_ctrl),]

# Calculate the bins
AbundW_5 <- Abund5 %>% mutate(bin = cut(log2mNG_final_ctrl, breaks = 5))

Bins <- rbind((AbundW_5 %>% filter(bin == "(-2.33,0.129]") %>% mutate(Bin = 1)),
             (AbundW_5 %>% filter(bin == "(0.129,2.57]") %>% mutate(Bin = 2)),
             (AbundW_5 %>% filter(bin == "(2.57,5.01]") %>% mutate(Bin = 3)),
             (AbundW_5 %>% filter(bin == "(5.01,7.46]") %>% mutate(Bin = 4)),
             (AbundW_5 %>% filter(bin == "(7.46,9.91]") %>% mutate(Bin = 5))) %>%
      select(ORF, gene, Category, Bin)

#Calculate how many are in each bin
AbundWT_5 <- AbundW_5 %>% select(c(Category, bin)) %>% group_by(Category, bin) %>% count()
AbundWT_5$Category <- factor(AbundWT_5$Category,levels = c("Complete degradation","Partial degradation","No degradation"))

#Figure 2e
Im2e3 <- ggplot(data = AbundWT_5) +
  geom_bar(aes(x = bin, y = n, fill = Category), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  labs(x = "Bin with equal width of Protein Abundance in log2 scale",
       y = "Percentage of ORFs at each bin")

print(Im2e3)

ggsave("05_plots/08-Figure_2e.pdf",
       height = 8,
    width = 12)


#Figure S2d
ImS2d <- ggplot(Categorizationp5) +
  geom_density(aes(x = log2mNG_final_ctrl)) +
  geom_vline(xintercept = c(-2.33, 0.129, 2.57, 5.01, 7.46, 9.91)) +
  scale_x_continuous(limits = c(-10,10))

print(ImS2d)

ggsave("05_plots/09-Figure_S2d.pdf",
       height = 8,
    width = 12)

```

# Size Measurements analysis

## Scatter plots

A list of essential genes is loaded in to compare with the hits. The expectation is that most, if not all, of the essential genes should have cause a significant growth defect in the library.


```{r}
#Use this to plot because the NA colonies are considered zero
df_spread_final_1 <- df_norm_3  %>%
  group_by(ORF, Fbox) %>%
  summarise(size = median(pnorm3, na.rm = T)) %>%
  spread(key = Fbox, value = size)

colnames(df_spread_final_1)[2] <- "ctrl"
colnames(df_spread_final_1)[3] <- "IAA"

df_spread_final_1 <- df_spread_final_1 %>% drop_na()


#For comparing with Library v2

df_spread_final_comp <- df_spread_final_1

colnames(df_spread_final_comp)[2] <- "ctrl_mNG"
colnames(df_spread_final_comp)[3] <- "IAA_mNG"

```


```{r Scatter plots with the normalised size}

# Scatter plot


#Essential Gemes
Scatter1 <- rbind((anti_join(df_spread_final_1, Ess_Genes_inLib , by = "ORF") %>% mutate(Gene = "Not Essential")), (semi_join(df_spread_final_1, Ess_Genes_inLib , by = "ORF") %>% mutate(Gene = "Essential")))


#Figure 3b   
Im3b <- ggplot(Scatter1) +
      geom_point(aes(ctrl,IAA, color=Gene), alpha = 0.3, size = 2)+
        theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
        scale_x_continuous(limits = c(0,1.5)) +
      labs(x = "Normalized colony size - control",
           y = "Normalized colony size - 5-Ph-IAA")

print(Im3b)

ggsave("05_plots/10-Figure_3b.pdf",
       height = 8,
    width = 12)      
      
ggplot(Scatter1) +
  geom_density(aes(x = ctrl, group = Gene, fill = Gene))

ggplot(Scatter1) +
  geom_density(aes(x = IAA, group = Gene, fill = Gene))
      



#Fitness Defect
Scatter5 <- rbind((anti_join(df_spread_final_1, Hits_IAA_d1_quot , by = "ORF") %>% mutate(Gene = "No hit")), (semi_join(df_spread_final_1, Hits_IAA_d1_quot , by = "ORF") %>% mutate(Gene = "Hit")))
Scatter5 <- Scatter5[!duplicated(Scatter5$ORF),]

#Figure 3a
Im3a <- ggplot(Scatter5) +
      geom_point(aes(ctrl, IAA, color=Gene), alpha = 0.3, size = 2) +
        theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
      scale_x_continuous(limits = c(0,1.5)) +
      labs(x = "Normalized colony size - control",
           y = "Normalized colony size - 5-Ph-IAA")

print(Im3a)

ggsave("05_plots/11-Figure_3a.pdf",
       height = 8,
    width = 12)


```

There are essential genes where the normalised size does not change after the treatment with the Auxin (red cloud in the middle of the cluster). A problem we can foresee with these graphics is that all the slow growth strains that do not change the size between control and query (that are found in the diagonal of the graphic) are considered hits. What we could do is to try to ignore all the hits that have, for example, a normalised size below 0.75.


## Protein Localization

We want to see now what is the protein localization of the hits and the ones that were not hits. For this, a database published in the paper https://www.nature.com/articles/nature02026. The dataset was downloaded at 12/09/2022. 

```{r}
Prot_Loc <- read.xlsx(xlsxFile = "06_datasets/20220912_GFPLocalizationLibrary.xlsx", sheet = "allOrfData", sep.names = " ") %>%
  `colnames<-`(c("orfid", colnames(.)[2:32]))
Prot_Loc_inLib <- semi_join(Prot_Loc, df_final, by = "ORF")
Prot_Loc_Ess_inLib <- semi_join(Prot_Loc_inLib, Ess_Genes_inLib, by = "ORF")

```



```{r Protein Localization2}

Ess_Genes_inLib <- semi_join(Ess_Genes, df_spread_final_1, by = "ORF")

Ess_Hits_quot <- rbind((semi_join(Ess_Genes_inLib, Hits_IAA_d1_quot, by = "ORF") %>% mutate(Category = "Hit")),(anti_join(Ess_Genes_inLib, Hits_IAA_d1_quot, by = "ORF") %>% mutate(Category = "Not Hit")) )



#Attribute each ORF to a cell localization
ORF_Loc_quot <- left_join(Ess_Hits_quot[c("ORF")], Prot_Loc_inLib, by = "ORF", suffix = c(".x",".y"))
ORF_Loc_quot <- as.data.frame(ORF_Loc_quot)

#Remove duplicated rows
ORF_Loc_quot <- ORF_Loc_quot[!duplicated(ORF_Loc_quot$ORF),]
rownames(ORF_Loc_quot) <- ORF_Loc_quot$ORF

#Quantify how many of each cell localization there is per Category
cell_loc_quot <- ORF_Loc_quot[10:length(colnames(ORF_Loc_quot))]
cell_loc_quot <- cell_loc_quot %>% mutate(across(everything(), as.logical))
cell_loc_quot$ORF <- rownames(cell_loc_quot)
cell_loc_quot <- left_join(cell_loc_quot, Ess_Hits_quot[c("ORF", "Category")], by="ORF")
cell_loc_quot <- cell_loc_quot %>% select(-c(ORF)) %>% group_by(Category) %>% summarize_all(sum, na.rm=TRUE) 
colSums(cell_loc_quot %>% select(-c( Category)), na.rm = TRUE)


cell_loc_perc_quot <- adorn_percentages(cell_loc_quot, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(cell_loc_quot, caption = "Localization of every ORF by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(cell_loc_perc_quot, caption = "Localization of every ORF by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
cell_loc_plot_quot <- cell_loc_perc_quot %>% pivot_longer(!Category, names_to = "cellular_comp", values_to = "amount")

#Plot
ggplot(data = cell_loc_plot_quot) +
  geom_col(aes(x = cellular_comp, y = amount, fill = Category), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category",
       title = "Percentage of Essential Genes in different cellular compartments")

ggplot(data = cell_loc_plot_quot) +
  geom_bar(aes(x = cellular_comp, y = amount,  fill = Category), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category",
       title = "Percentage of Essential Genes in different cellular compartments")


#Figure 3d
Im3d <- ggplot(data = cell_loc_plot_quot) +
  geom_bar(aes(x = cellular_comp, y = amount,  fill = Category), position = "fill", stat = "identity") +
theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category")

print(Im3d)

ggsave("05_plots/12-Figure_3d.pdf",
       height = 8,
    width = 12)


```

The analysis was also performed to the Non Essential dataset.

```{r Protein Localization all Dataset}

Non_Ess_Genes_in_Lib <- anti_join(df_spread_final_1,Ess_Genes, by = "ORF")


Non_Ess_Hits_quot <- rbind((semi_join(Non_Ess_Genes_in_Lib, Hits_IAA_d1_quot, by = "ORF") %>% mutate(Category = "Hit")),(anti_join(Non_Ess_Genes_in_Lib, Hits_IAA_d1_quot, by = "ORF") %>% mutate(Category = "Not Hit")) )


#Attribute each ORF to a cell localization
ORF_Loc_quot <- left_join(Non_Ess_Hits_quot[c("ORF")], Prot_Loc_inLib, by = "ORF", suffix = c(".x",".y"))
ORF_Loc_quot <- as.data.frame(ORF_Loc_quot)

#Remove duplicated rows
ORF_Loc_quot <- ORF_Loc_quot[!duplicated(ORF_Loc_quot$ORF),]
rownames(ORF_Loc_quot) <- ORF_Loc_quot$ORF

#Quantify how many of each cell localization there is per Category
cell_loc_quot <- ORF_Loc_quot[10:length(colnames(ORF_Loc_quot))]
cell_loc_quot <- cell_loc_quot %>% mutate(across(everything(), as.logical))
cell_loc_quot$ORF <- rownames(cell_loc_quot)
cell_loc_quot <- left_join(cell_loc_quot, Non_Ess_Hits_quot[c("ORF", "Category")], by="ORF")
cell_loc_quot <- cell_loc_quot %>% select(-c(ORF)) %>% group_by(Category) %>% summarize_all(sum, na.rm=TRUE) 
colSums(cell_loc_quot %>% select(-c( Category)), na.rm = TRUE)


cell_loc_perc_quot <- adorn_percentages(cell_loc_quot, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(cell_loc_quot, caption = "Localization of every ORF by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(cell_loc_perc_quot, caption = "Localization of every ORF by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
cell_loc_plot_quot <- cell_loc_perc_quot %>% pivot_longer(!Category, names_to = "cellular_comp", values_to = "amount")

#Plot
ggplot(data = cell_loc_plot_quot) +
  geom_col(aes(x = cellular_comp, y = amount, fill = Category), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category",
       title = "Percentage of Essential Genes in different cellular compartments")

ggplot(data = cell_loc_plot_quot) +
  geom_bar(aes(x = cellular_comp, y = amount,  fill = Category), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category",
       title = "Percentage of Essential Genes in different cellular compartments")

#Figure 3d
Im3d2 <- ggplot(data = cell_loc_plot_quot) +
  geom_bar(aes(x = cellular_comp, y = amount,  fill = Category), position = "fill", stat = "identity") +
theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "Cellular Components",
       y = "Percentage of ORFs at each Category")

print(Im3d2)

ggsave("05_plots/13-Figure_3d.pdf",
       height = 8,
    width = 12)


```


## C-terminal domain availability in membrane protein


We want to investigate if the hits are related with the fact that the C-terminal domain is available or not to be recognize by the degradation system. For that, the dataset of hit was compared with a dataset of membrane protein were the C-terminal domain are known to be cytosolic of in the lumen. The dataset was downloaded at 2022-09-13 in the website https://www.pnas.org/doi/10.1073/pnas.0604075103.

```{r C-terminal availability in the membrane proteins}

C_terminal_domain <- read.xlsx(xlsxFile = "06_datasets/20220913_CterminalLocalization.xlsx", sheet = "Table 2 Data")
C_terminal_domain_inLib <- semi_join(C_terminal_domain, df_final, by = "ORF") %>% 
  select(ORF,Gene.name,Cterm)

```

For Essential Genes

```{r C-terminal availability in the membrane proteins 2}

#Attribute each ORF to a cell localization
ORF_Cterm_quot <- inner_join(Ess_Hits_quot[c("ORF")], C_terminal_domain_inLib, by = "ORF", suffix = c(".x",".y"))
ORF_Cterm_quot <- as.data.frame(ORF_Cterm_quot)

#Remove duplicated rows
ORF_Cterm_quot <- ORF_Cterm_quot[!duplicated(ORF_Cterm_quot$ORF),]
rownames(ORF_Cterm_quot) <- ORF_Cterm_quot$ORF

#Filter ORFs with no information

ORF_Cterm_quot <- ORF_Cterm_quot %>% filter(Cterm != "-")

#Quantify how many of each cell localization there is per Category
ORF_Cterm_quot$Cterm[ORF_Cterm_quot$Cterm == "in*"] <- "in"

ORF_Cterm_quot$Cytosol <- ifelse(ORF_Cterm_quot$Cterm == "in", TRUE, FALSE)
ORF_Cterm_quot$Lumen <- ifelse(ORF_Cterm_quot$Cterm != "in", TRUE, FALSE)




cterm_quot <- left_join(ORF_Cterm_quot, Ess_Hits_quot[c("ORF", 'Category')], by="ORF")
cterm_quot <- cterm_quot %>% select(-c(ORF,Gene.name,Cterm)) %>% group_by(Category) %>% summarize_all(sum, na.rm=TRUE) 
colSums(cterm_quot %>% select(-c( Category)), na.rm = TRUE)


#Calculate the percentage of each cell localization per Category
cterm_perc_quot <- adorn_percentages(cterm_quot, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(cterm_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(cterm_perc_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
cterm_plot_quot <- cterm_perc_quot %>% pivot_longer(!Category, names_to = "Membrane_pos", values_to = "amount")

#Plot
ggplot(data = cterm_plot_quot) +
  geom_col(aes(x = Membrane_pos, y = amount, fill = Category), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")


ggplot(data = cterm_plot_quot) +
  geom_bar(aes(x = Membrane_pos, y = amount, fill = Category), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")



#Figure 3d
Im3d3 <- ggplot(data = cterm_plot_quot) +
  geom_bar(aes(x = Membrane_pos, y = amount, fill = Category), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category")

print(Im3d3)

ggsave("05_plots/14-Figure_3d.pdf",
       height = 8,
    width = 12)




```

As before, the same analysis was applied Non Essential dataset.

```{r C-terminal availability in the membrane proteins all}


#Attribute each ORF to a cell localization
ORF_Cterm_quot <- inner_join(Non_Ess_Hits_quot[c("ORF")], C_terminal_domain_inLib, by = "ORF", suffix = c(".x",".y"))
ORF_Cterm_quot <- as.data.frame(ORF_Cterm_quot)

#Remove duplicated rows
ORF_Cterm_quot <- ORF_Cterm_quot[!duplicated(ORF_Cterm_quot$ORF),]
rownames(ORF_Cterm_quot) <- ORF_Cterm_quot$ORF

#Filter ORFs with no information

ORF_Cterm_quot <- ORF_Cterm_quot %>% filter(Cterm != "-")

#Quantify how many of each cell localization there is per Category
ORF_Cterm_quot$Cterm[ORF_Cterm_quot$Cterm == "in*"] <- "in"

ORF_Cterm_quot$Cytosol <- ifelse(ORF_Cterm_quot$Cterm == "in", TRUE, FALSE)
ORF_Cterm_quot$Lumen <- ifelse(ORF_Cterm_quot$Cterm != "in", TRUE, FALSE)




cterm_quot <- left_join(ORF_Cterm_quot, Non_Ess_Hits_quot[c("ORF", 'Category')], by="ORF")
cterm_quot <- cterm_quot %>% select(-c(ORF,Gene.name,Cterm)) %>% group_by(Category) %>% summarize_all(sum, na.rm=TRUE) 
colSums(cterm_quot %>% select(-c( Category)), na.rm = TRUE)


#Calculate the percentage of each cell localization per Category
cterm_perc_quot <- adorn_percentages(cterm_quot, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(cterm_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(cterm_perc_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
cterm_plot_quot <- cterm_perc_quot %>% pivot_longer(!Category, names_to = "Membrane_pos", values_to = "amount")

#Plot
ggplot(data = cterm_plot_quot) +
  geom_col(aes(x = Membrane_pos, y = amount, fill = Category), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")


ggplot(data = cterm_plot_quot) +
  geom_bar(aes(x = Membrane_pos, y = amount, fill = Category), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")



#Figure 3d
Im3d4 <- ggplot(data = cterm_plot_quot) +
  geom_bar(aes(x = Membrane_pos, y = amount, fill = Category), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "C-terminal Position of the ORF",
       y = "Percentage of Membrane ORFs at each Category")

print(Im3d4)

ggsave("05_plots/15-Figure_3d.pdf",
       height = 8,
    width = 12)




```


### Dividing data in bins

```{r}

bins <- Bins
bins <- as.data.frame(bins)

#Essential Genes
Hits_Ess_Bins <- rbind((semi_join(Ess_Hits_quot, (bins %>% filter(Bin == 1)), by = "ORF") %>% mutate(Bin = 1)),
                       (semi_join(Ess_Hits_quot, (bins %>% filter(Bin == 2)), by = "ORF") %>% mutate(Bin = 2)),
                       (semi_join(Ess_Hits_quot, (bins %>% filter(Bin == 3)), by = "ORF") %>% mutate(Bin = 3)),
                       (semi_join(Ess_Hits_quot, (bins %>% filter(Bin == 4)), by = "ORF") %>% mutate(Bin = 4)),
                       (semi_join(Ess_Hits_quot, (bins %>% filter(Bin == 5)), by = "ORF") %>% mutate(Bin = 5)))

Hits_Ess_BinsWT <- Hits_Ess_Bins %>% select(c(Category, Bin)) %>% group_by(Category, Bin) %>% count()
Hits_Ess_BinsWT$Category <- factor(Hits_Ess_BinsWT$Category,levels = c("Hit","Not Hit"))

#Figure 3d
Im3d5 <- ggplot(data = Hits_Ess_BinsWT) +
  geom_bar(aes(x = Bin, y = n, fill = Category), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  labs(x = "Bin with equal width of Protein Abundance in log2 scale from mNG data",
       y = "Percentage of ORFs at each bin")

print(Im3d5)

ggsave("05_plots/16-Figure_3d.pdf",
       height = 8,
    width = 12)



```

```{r}

#Non Essential Genes
Hits_Bins <- rbind((semi_join(Non_Ess_Hits_quot, (bins %>% filter(Bin == 1)), by = "ORF") %>% mutate(Bin = 1)),
                       (semi_join(Non_Ess_Hits_quot, (bins %>% filter(Bin == 2)), by = "ORF") %>% mutate(Bin = 2)),
                       (semi_join(Non_Ess_Hits_quot, (bins %>% filter(Bin == 3)), by = "ORF") %>% mutate(Bin = 3)),
                       (semi_join(Non_Ess_Hits_quot, (bins %>% filter(Bin == 4)), by = "ORF") %>% mutate(Bin = 4)),
                       (semi_join(Non_Ess_Hits_quot, (bins %>% filter(Bin == 5)), by = "ORF") %>% mutate(Bin = 5)))

Hits_BinsWT <- Hits_Bins %>% select(c(Category, Bin)) %>% group_by(Category, Bin) %>% count()
Hits_BinsWT$Category <- factor(Hits_BinsWT$Category,levels = c("Hit","Not Hit"))


#Figure 3d
Im3d6 <- ggplot(data = Hits_BinsWT) +
  geom_bar(aes(x = Bin, y = n, fill = Category), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  labs(x = "Bin with equal width of Protein Abundance in log2 scale from mNG data",
       y = "Percentage of ORFs at each bin")

print(Im3d6)

ggsave("05_plots/17-Figure_3d.pdf",
       height = 8,
    width = 12)



```


## Stratification of the data in Essential and Non-Essential Hits

```{r Stratification of the data - Essential}



#Attribute each ORF to a category
ORF_Ess_Cat_quot <- inner_join(Ess_Hits_quot[c("ORF")], Categorizationp5[c("ORF","Category")], by = "ORF", suffix = c(".x",".y"))
ORF_Ess_Cat_quot <- as.data.frame(ORF_Ess_Cat_quot)

#Remove duplicated rows
ORF_Ess_Cat_quot <- ORF_Ess_Cat_quot[!duplicated(ORF_Ess_Cat_quot$ORF),]
rownames(ORF_Ess_Cat_quot) <- ORF_Ess_Cat_quot$ORF


#Quantify how many of each cell localization there is per Category

ORF_Ess_Cat_quot$Complete_degradation <- ifelse(ORF_Ess_Cat_quot$Category == "Complete degradation", TRUE, FALSE)
ORF_Ess_Cat_quot$Partial_degradation <- ifelse(ORF_Ess_Cat_quot$Category == "Partial degradation", TRUE, FALSE)
ORF_Ess_Cat_quot$No_degradation <- ifelse(ORF_Ess_Cat_quot$Category == "No degradation", TRUE, FALSE)
ORF_Ess_Cat_quot$Not_Detectable <- ifelse(ORF_Ess_Cat_quot$Category == "Not detectable", TRUE, FALSE)




ess_cat_quot <- left_join(ORF_Ess_Cat_quot, Ess_Hits_quot[c("ORF", 'Category')], by="ORF")
ess_cat_quot <- ess_cat_quot %>% select(-c(ORF,Category.x)) %>% group_by(Category.y) %>% summarize_all(sum, na.rm=TRUE) 
colSums(ess_cat_quot %>% select(-c( Category.y)), na.rm = TRUE)


#Calculate the percentage of each cell localization per Category
ess_cat_perc_quot <- adorn_percentages(ess_cat_quot, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(ess_cat_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(ess_cat_perc_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
ess_cat_plot_quot <- ess_cat_perc_quot %>% pivot_longer(!Category.y, names_to = "Category", values_to = "amount")

#Plot
ggplot(data = ess_cat_plot_quot) +
  geom_col(aes(x = Category, y = amount, fill = Category.y), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Category",
       y = "Percentage of Hits at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")


ggplot(data = ess_cat_plot_quot) +
  geom_bar(aes(x = Category, y = amount, fill = Category.y), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Category",
       y = "Percentage of Hits at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")



#Figure 3c
Im3c <- ggplot(data = ess_cat_plot_quot) +
  geom_bar(aes(x = Category, y = amount, fill = Category.y), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "Category",
       y = "Percentage of Hits at each Category")

print(Im3c)

ggsave("05_plots/18-Figure_3c.pdf",
       height = 8,
    width = 12)




```

```{r Stratification of the data - Non Essential}



#Attribute each ORF to a category
ORF_Non_Ess_Cat_quot <- inner_join(Non_Ess_Hits_quot[c("ORF")],  Categorizationp5[c("ORF","Category")], by = "ORF", suffix = c(".x",".y"))
ORF_Non_Ess_Cat_quot <- as.data.frame(ORF_Non_Ess_Cat_quot)

#Remove duplicated rows
ORF_Non_Ess_Cat_quot <- ORF_Non_Ess_Cat_quot[!duplicated(ORF_Non_Ess_Cat_quot$ORF),]
rownames(ORF_Non_Ess_Cat_quot) <- ORF_Non_Ess_Cat_quot$ORF


#Quantify how many of each cell localization there is per Category

ORF_Non_Ess_Cat_quot$Complete_degradation <- ifelse(ORF_Non_Ess_Cat_quot$Category == "Complete degradation", TRUE, FALSE)
ORF_Non_Ess_Cat_quot$Partial_degradation <- ifelse(ORF_Non_Ess_Cat_quot$Category == "Partial degradation", TRUE, FALSE)
ORF_Non_Ess_Cat_quot$No_degradation <- ifelse(ORF_Non_Ess_Cat_quot$Category == "No degradation", TRUE, FALSE)
ORF_Non_Ess_Cat_quot$Not_Detectable <- ifelse(ORF_Non_Ess_Cat_quot$Category == "Not detectable", TRUE, FALSE)




non_ess_cat_quot <- left_join(ORF_Non_Ess_Cat_quot, Non_Ess_Hits_quot[c("ORF", 'Category')], by="ORF")
non_ess_cat_quot <- non_ess_cat_quot %>% select(-c(ORF,Category.x)) %>% group_by(Category.y) %>% summarize_all(sum, na.rm=TRUE) 
colSums(non_ess_cat_quot %>% select(-c( Category.y)), na.rm = TRUE)


#Calculate the percentage of each cell localization per Category
non_ess_cat_perc_quot <- adorn_percentages(non_ess_cat_quot, denominator = "col", na.rm = TRUE)

#Tables
knitr::kable(non_ess_cat_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

knitr::kable(non_ess_cat_quot, caption = "C-terminal Position of the ORF in the Cellular Membrane by Category in percentage", output = T) %>%
  kable_styling(c("striped", "bordered"), full_width = TRUE, position = "left")

#Transform dataset for plotting
non_ess_cat_plot_quot <- non_ess_cat_perc_quot %>% pivot_longer(!Category.y, names_to = "Category", values_to = "amount")

#Plot
ggplot(data = non_ess_cat_plot_quot) +
  geom_col(aes(x = Category, y = amount, fill = Category.y), position = position_dodge2()) +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Category",
       y = "Percentage of Hits at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")


ggplot(data = non_ess_cat_plot_quot) +
  geom_bar(aes(x = Category, y = amount, fill = Category.y), position = "fill", stat = "identity") +
#  geom_label(aes(x = cellular_comp, y = amount_per, label = amount_per, fill = Hits_rep), label.size = 0.25, position = position_dodge2(width = 1)) +
  coord_flip() +
  labs(x = "Category",
       y = "Percentage of Hits at each Category",
       title = "C-terminal Position of the Essential Genes in the Cellular Membrane")



#Figure 3c
Im3c2 <- ggplot(data = non_ess_cat_plot_quot) +
  geom_bar(aes(x = Category, y = amount, fill = Category.y), position = "fill", stat = "identity") +
  theme_light(base_size = 20) +
      theme(legend.text = element_text(size = 18),
            legend.title = element_text(size = 16))+ 
      guides(colour = guide_legend(override.aes = list(size=5))) +
  coord_flip() +
  labs(x = "Category",
       y = "Percentage of Hits at each Category")

print(Im3c2)

ggsave("05_plots/19-Figure_3c.pdf",
       height = 8,
    width = 12)



```

# Preparing datasets for export

```{r}

Final_Library_v1_size_w_Hits <- rbind((semi_join(Library_v1_final, Hits_IAA_d1_quot, by = "ORF") %>% mutate(Screen = "Hit")), (anti_join(Library_v1_final, Hits_IAA_d1_quot, by = "ORF") %>% mutate(Screen = "Not Hit")))

Description <- read.xlsx(xlsxFile = "06_datasets/Description.xlsx", sheet = "Description") %>% select(-Gene)

Final_Library_v1_size_w_Hits <- left_join(Final_Library_v1_size_w_Hits,Description, by = "ORF")

```



# R Studio Session Info

```{r Session info}
sessionInfo()


output_folder <- "07_output/"


write.xlsx(
  x = list("Categorization_Fluorescence" = Categorizationp5,
           "Size" = Final_Library_v1_size_w_Hits),
  file = paste0(output_folder, "Library_v1_final_table.xlsx")
)



```

